{"cells":[{"cell_type":"code","execution_count":null,"id":"2cf0af11","metadata":{"vscode":{"languageId":"plaintext"},"id":"2cf0af11"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","# **TUNED HYPERPARAMETER**: Changed time_step to 90\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train the Model with Cross-Validation ---\n","\n","# Use TimeSeriesSplit for robust cross-validation on time-series data.\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","# Store scores and histories from each fold\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Bidirectional LSTM Model ---\n","    # This architecture can capture patterns from both past-to-future and future-to-past.\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[2])),\n","        Bidirectional(LSTM(150, return_sequences=True)), # Using Bidirectional wrapper\n","        Dropout(0.2),\n","        Bidirectional(LSTM(150, return_sequences=True)), # Using Bidirectional wrapper\n","        Dropout(0.2),\n","        Bidirectional(LSTM(64)), # Using Bidirectional wrapper\n","        Dropout(0.2),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # --- Use Early Stopping to Prevent Overfitting ---\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=20,\n","        verbose=1,\n","        restore_best_weights=True\n","    )\n","\n","    # --- Train the Model ---\n","    history = model.fit(\n","        X_train,\n","        y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100,\n","        batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","\n","    # --- Evaluate on the test fold ---\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    # Store results for final evaluation\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Model on All Data for Prediction ---\")\n","\n","# --- Handle Class Imbalance for the full dataset ---\n","final_class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_seq),\n","    y=y_seq\n",")\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","print(\"Final Class Weights:\", final_class_weights_dict)\n","\n","# --- Build the final model with Bidirectional LSTMs ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[2])),\n","    Bidirectional(LSTM(150, return_sequences=True)),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(150, return_sequences=True)),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(64)),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model on the entire dataset ---\n","final_model.fit(\n","    X_seq,\n","    y_seq,\n","    epochs=30, # A reasonable number of epochs based on previous runs\n","    batch_size=64,\n","    class_weight=final_class_weights_dict,\n","    verbose=1\n",")\n","\n","# --- Prepare the last 90 days of data for prediction ---\n","last_days = X_scaled[-time_step:]\n","prediction_input = last_days.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make the prediction ---\n","prediction_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(prediction_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"},"colab":{"base_uri":"https://localhost:8080/"},"id":"gTT-CPl-odka","outputId":"10372e6f-8e64-4948-8b54-69fcffa20d94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data loaded and merged successfully.\n","DataFrame shape: (3842, 144)\n","\n","--- Performing Advanced Feature Engineering ---\n","Shape after feature engineering: (3813, 267)\n","\n","--- Performing Feature Selection with XGBoost ---\n","Selected Top 75 features.\n","\n","--- Class Distribution Analysis ---\n","Number of unique crisis levels (classes): 4\n","Unique target values: [0 1 2 3]\n","\n","Overall class distribution in the entire dataset:\n","Crisis_Target_V2\n","0    1295\n","1    1644\n","2     747\n","3     127\n","Name: count, dtype: int64\n","\n","--- Starting 5-Fold Time-Series Cross-Validation with Optimized Bi-GRU Model ---\n","\n","===== FOLD 1/5 =====\n","Class Weights for this fold: {0: np.float64(1.2818930041152263), 1: np.float64(0.5165837479270315), 2: np.float64(3.519774011299435)}\n","Epoch 1/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.4456 - loss: 0.9963 - val_accuracy: 0.7403 - val_loss: 0.9237\n","Epoch 2/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 848ms/step - accuracy: 0.8310 - loss: 0.3521 - val_accuracy: 0.6758 - val_loss: 1.2783\n","Epoch 3/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 848ms/step - accuracy: 0.9288 - loss: 0.1824 - val_accuracy: 0.6613 - val_loss: 1.5642\n","Epoch 4/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 985ms/step - accuracy: 0.9735 - loss: 0.0838 - val_accuracy: 0.6919 - val_loss: 1.3619\n","Epoch 5/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 854ms/step - accuracy: 0.9737 - loss: 0.0640 - val_accuracy: 0.6935 - val_loss: 1.1738\n","Epoch 6/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 976ms/step - accuracy: 0.9550 - loss: 0.1103 - val_accuracy: 0.6419 - val_loss: 1.2839\n","Epoch 7/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 967ms/step - accuracy: 0.9717 - loss: 0.0751 - val_accuracy: 0.6306 - val_loss: 1.4172\n","Epoch 8/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 849ms/step - accuracy: 0.9718 - loss: 0.0815 - val_accuracy: 0.6048 - val_loss: 1.5060\n","Epoch 9/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 974ms/step - accuracy: 0.9685 - loss: 0.0597 - val_accuracy: 0.5677 - val_loss: 1.6859\n","Epoch 10/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 825ms/step - accuracy: 0.9728 - loss: 0.0620 - val_accuracy: 0.6048 - val_loss: 1.5157\n","Epoch 11/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 969ms/step - accuracy: 0.9585 - loss: 0.0710 - val_accuracy: 0.5919 - val_loss: 1.6271\n","Epoch 12/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 970ms/step - accuracy: 0.9836 - loss: 0.0495 - val_accuracy: 0.5903 - val_loss: 1.7891\n","Epoch 13/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 973ms/step - accuracy: 0.9823 - loss: 0.0683 - val_accuracy: 0.6210 - val_loss: 1.4069\n","Epoch 14/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 970ms/step - accuracy: 0.9852 - loss: 0.0434 - val_accuracy: 0.6371 - val_loss: 1.2196\n","Epoch 15/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 977ms/step - accuracy: 0.9863 - loss: 0.0394 - val_accuracy: 0.6129 - val_loss: 1.6177\n","Epoch 16/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 925ms/step - accuracy: 0.9680 - loss: 0.0605 - val_accuracy: 0.6274 - val_loss: 1.4492\n","Epoch 17/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 968ms/step - accuracy: 0.9786 - loss: 0.0456 - val_accuracy: 0.5935 - val_loss: 1.5589\n","Epoch 18/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 974ms/step - accuracy: 0.9811 - loss: 0.0653 - val_accuracy: 0.6677 - val_loss: 1.2784\n","Epoch 19/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 845ms/step - accuracy: 0.9723 - loss: 0.0548 - val_accuracy: 0.6581 - val_loss: 1.0905\n","Epoch 20/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 841ms/step - accuracy: 0.9670 - loss: 0.0636 - val_accuracy: 0.5452 - val_loss: 2.1048\n","Epoch 21/150\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 929ms/step - accuracy: 0.9714 - loss: 0.0558 - val_accuracy: 0.5710 - val_loss: 2.0106\n","Epoch 21: early stopping\n","Restoring model weights from the end of the best epoch: 1.\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step\n","\n","Optimized Bi-GRU Accuracy for Fold 1: 0.7403\n","\n","===== FOLD 2/5 =====\n","Class Weights for this fold: {0: np.float64(1.253024193548387), 1: np.float64(0.46173848439821696), 2: np.float64(1.1178057553956835), 3: np.float64(7.0625)}\n","Epoch 1/150\n","\u001b[1m12/39\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 773ms/step - accuracy: 0.2060 - loss: 1.6088"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, GRU\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Advanced Feature Engineering ---\n","print(\"\\n--- Performing Advanced Feature Engineering ---\")\n","\n","# Make a copy to avoid SettingWithCopyWarning\n","X_df_full = df.drop('Crisis_Target_V2', axis=1).copy()\n","y_df = df['Crisis_Target_V2']\n","\n","# Create a list to hold new feature DataFrames to avoid fragmentation\n","new_features_list = []\n","\n","# a. Reservoir Capacity Percentage\n","reservoirs = ['CHEMBARAMBAKKAM', 'CHOLAVARAM', 'POONDI', 'PUZHAL', 'VEERANAM', 'KANNANKOTTAI_THERVOY_KANDIGAI']\n","temp_df = pd.DataFrame(index=X_df_full.index)\n","for res in reservoirs:\n","    storage_col = f'Storage_mcft_{res}'\n","    capacity_col = f'Full_Capacity_mcft_{res}'\n","    if storage_col in X_df_full.columns and capacity_col in X_df_full.columns:\n","        temp_df[f'Capacity_perc_{res}'] = (X_df_full[storage_col] / (X_df_full[capacity_col] + 1e-6)) * 100\n","new_features_list.append(temp_df)\n","\n","\n","# b. Lag Features for all reservoirs and key weather metrics\n","lag_features = []\n","for res in reservoirs:\n","    lag_features.append(f'Rainfall_mm_{res}')\n","    lag_features.append(f'Inflow_cusecs_{res}')\n","lag_features.extend(['temperature_mean_celsius', 'relative_humidity_mean_percent'])\n","lags = [1, 3, 7]\n","temp_df = pd.DataFrame(index=X_df_full.index)\n","for feature in lag_features:\n","    if feature in X_df_full.columns:\n","        for lag in lags:\n","            temp_df[f'{feature}_lag_{lag}'] = X_df_full[feature].shift(lag)\n","new_features_list.append(temp_df)\n","\n","\n","# c. Rolling Averages & Sums for all major reservoirs\n","rolling_features = []\n","for res in reservoirs:\n","    rolling_features.append(f'Rainfall_mm_{res}')\n","    rolling_features.append(f'Inflow_cusecs_{res}')\n","windows = [7, 14, 30]\n","temp_df = pd.DataFrame(index=X_df_full.index)\n","for feature in rolling_features:\n","    if feature in X_df_full.columns:\n","        for window in windows:\n","            temp_df[f'{feature}_roll_mean_{window}'] = X_df_full[feature].rolling(window=window).mean()\n","            temp_df[f'{feature}_roll_sum_{window}'] = X_df_full[feature].rolling(window=window).sum()\n","new_features_list.append(temp_df)\n","\n","\n","# d. Time-Based Features\n","temp_df = pd.DataFrame(index=X_df_full.index)\n","temp_df['month'] = X_df_full.index.month\n","temp_df['week_of_year'] = X_df_full.index.isocalendar().week.astype(int)\n","temp_df['day_of_year'] = X_df_full.index.dayofyear\n","new_features_list.append(temp_df)\n","\n","# e. Interaction Features\n","if 'temperature_mean_celsius' in X_df_full.columns and 'relative_humidity_mean_percent' in X_df_full.columns:\n","    temp_df = pd.DataFrame(index=X_df_full.index)\n","    temp_df['temp_humidity_interaction'] = X_df_full['temperature_mean_celsius'] / (X_df_full['relative_humidity_mean_percent'] + 1e-6)\n","    new_features_list.append(temp_df)\n","\n","# Concatenate all new features at once for efficiency\n","X_df_full = pd.concat([X_df_full] + new_features_list, axis=1)\n","\n","\n","# Drop rows with NaN values created by feature engineering\n","X_df_full.dropna(inplace=True)\n","y_df = y_df.loc[X_df_full.index]\n","\n","print(f\"Shape after feature engineering: {X_df_full.shape}\")\n","\n","\n","# --- 3. Intelligent Feature Selection using XGBoost ---\n","print(\"\\n--- Performing Feature Selection with XGBoost ---\")\n","\n","# Train a simple XGBoost model to get feature importances\n","xgb_selector = xgb.XGBClassifier(objective='multi:softmax', eval_metric='mlogloss')\n","xgb_selector.fit(X_df_full.values, y_df.values) # Pass NumPy arrays here\n","\n","# Get feature importances\n","importances = xgb_selector.feature_importances_\n","feature_names = X_df_full.columns\n","\n","# Create a DataFrame for visualization\n","feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n","feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n","\n","# Select the top N features\n","N_FEATURES = 75\n","top_features = feature_importance_df.head(N_FEATURES)['feature'].tolist()\n","print(f\"Selected Top {N_FEATURES} features.\")\n","\n","# Create the final DataFrame with only the selected features\n","X_df = X_df_full[top_features]\n","\n","\n","# --- 4. Preprocess the Data ---\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Scale the selected features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 5. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 6. Build and Train Optimized Bidirectional GRU with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Optimized Bi-GRU Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Optimized Bidirectional GRU Model ---\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[-1])),\n","        Bidirectional(GRU(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(GRU(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(GRU(64)),\n","        Dropout(0.25),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    optimizer = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=150,\n","        batch_size=32,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nOptimized Bi-GRU Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 7. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Optimized Bi-GRU Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Optimized Bi-GRU Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 8. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Optimized Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[-1])),\n","    Bidirectional(GRU(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(GRU(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(GRU(64)),\n","    Dropout(0.25),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_optimizer = Adam(learning_rate=0.0005)\n","final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_model.fit(X_seq, y_seq, epochs=40, batch_size=32, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      OPTIMIZED GRU PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")"],"id":"gTT-CPl-odka"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-EAqqzsqPii","executionInfo":{"status":"ok","timestamp":1755595774949,"user_tz":-330,"elapsed":37775,"user":{"displayName":"Pranav Bhat","userId":"05304972169025204322"}},"outputId":"f51ef1fa-c8d1-4262-8c1b-42ef0a567715"},"id":"x-EAqqzsqPii","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}