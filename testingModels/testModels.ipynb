{"cells":[{"cell_type":"code","execution_count":null,"id":"3WNC__Nk9mVT","metadata":{"id":"3WNC__Nk9mVT"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"746d8a5f","metadata":{"id":"746d8a5f"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","\n","def build_and_train_lstm(input_filename, model_output_filename):\n","    \"\"\"\n","    Loads the feature-engineered dataset, preprocesses it for an LSTM,\n","    builds, trains, and saves a time-series classification model.\n","    \"\"\"\n","    try:\n","        # --- Step 1: Load and Prepare Data ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","\n","        # The 'Date' column is not needed for training, but useful for reference\n","        dates = pd.to_datetime(df['Date'])\n","        df = df.drop(columns=['Date'])\n","\n","        # Separate features (X) and the target variable (y)\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        print(\"Data loaded successfully.\")\n","        print(f\"Shape of features (X): {X.shape}\")\n","        print(f\"Shape of target (y): {y.shape}\")\n","\n","        # --- Step 2: Scale Features and One-Hot Encode Target ---\n","        print(\"\\nStep 2: Scaling features and encoding target...\")\n","\n","        # Scale all feature columns to a range of [0, 1]\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","\n","        # One-hot encode the target variable for classification (e.g., 2 -> [0, 0, 1, 0])\n","        # Assumes 4 crisis levels (0, 1, 2, 3)\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        # --- Step 3: Create Time-Series Sequences ---\n","        print(\"Step 3: Creating time-series sequences...\")\n","\n","        # We will use a sequence of the last 60 days to predict the next day\n","        sequence_length = 60\n","\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        print(f\"Created sequences. Shape of X: {X_sequences.shape}, Shape of y: {y_sequences.shape}\")\n","\n","        # Split data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","\n","        # --- Step 4: Build the LSTM Model Architecture ---\n","        print(\"\\nStep 4: Building the LSTM model...\")\n","\n","        model = Sequential()\n","\n","        # First LSTM layer with dropout\n","        # return_sequences=True is needed to pass the output to the next LSTM layer\n","        model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n","        model.add(Dropout(0.2))\n","\n","        # Second LSTM layer\n","        model.add(LSTM(units=100, return_sequences=True))\n","        model.add(Dropout(0.2))\n","\n","        # Third LSTM layer\n","        # return_sequences=False as this is the last LSTM layer\n","        model.add(LSTM(units=50))\n","        model.add(Dropout(0.2))\n","\n","        # Output Layer\n","        # The output layer has 4 neurons (for 4 crisis classes) and uses 'softmax' activation\n","        # for multi-class classification.\n","        model.add(Dense(units=4, activation='softmax'))\n","\n","        print(\"Model architecture defined.\")\n","        model.summary()\n","\n","        # --- Step 5: Compile the Model ---\n","        print(\"\\nStep 5: Compiling the model...\")\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        # --- Step 6: Train the Model ---\n","        print(\"\\nStep 6: Training the model...\")\n","        history = model.fit(\n","            X_train, y_train,\n","            epochs=50,  # Number of times to iterate over the entire dataset\n","            batch_size=32, # Number of samples per gradient update\n","            validation_data=(X_test, y_test),\n","            verbose=1\n","        )\n","\n","        print(\"\\nModel training complete.\")\n","\n","        # --- Step 7: Save the Trained Model ---\n","        print(f\"\\nStep 7: Saving the trained model to '{model_output_filename}'...\")\n","        model.save(model_output_filename)\n","        print(\"Model saved successfully.\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    # Using the feature-engineered dataset as input\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = 'chennai_water_crisis_lstm_model.h5'\n","\n","    build_and_train_lstm(input_file, output_file)"]},{"cell_type":"code","execution_count":null,"id":"19198222","metadata":{"id":"19198222"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def evaluate_lstm_model(input_filename, model_output_filename):\n","    \"\"\"\n","    Loads, preprocesses, builds, trains, and evaluates a time-series\n","    classification model, including generating a confusion matrix.\n","    \"\"\"\n","    try:\n","        # --- Steps 1-3: Load, Scale, and Create Sequences (Same as before) ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","        print(\"Data preparation complete.\")\n","\n","        # --- Step 4: Build and Train the LSTM Model (Same as before) ---\n","        print(\"\\nBuilding and training the LSTM model...\")\n","        model = Sequential([\n","            LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n","            Dropout(0.2),\n","            LSTM(units=100, return_sequences=True),\n","            Dropout(0.2),\n","            LSTM(units=50),\n","            Dropout(0.2),\n","            Dense(units=4, activation='softmax')\n","        ])\n","\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        model.fit(\n","            X_train, y_train,\n","            epochs=50,\n","            batch_size=32,\n","            validation_data=(X_test, y_test),\n","            verbose=1 # Set to 0 to hide epoch logs during training\n","        )\n","        print(\"Model training complete.\")\n","\n","        # --- Step 5: Evaluate the Model (NEW CODE) ---\n","        print(\"\\n--- Step 5: Evaluating Model Performance ---\")\n","\n","        # a) Make predictions on the test data\n","        y_pred_probs = model.predict(X_test)\n","\n","        # b) Convert predicted probabilities to class labels (0, 1, 2, or 3)\n","        #    np.argmax finds the index of the highest probability.\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","        # c) Convert the one-hot encoded true labels back to class labels\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        # d) Calculate and print the accuracy score\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"\\nModel Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        # --- Step 6: Generate and Plot the Confusion Matrix (NEW CODE) ---\n","        print(\"\\n--- Step 6: Generating Confusion Matrix ---\")\n","\n","        # a) Calculate the confusion matrix\n","        cm = confusion_matrix(y_true, y_pred)\n","\n","        # b) Plot the confusion matrix as a heatmap for better visualization\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","\n","        # Save the plot to a file\n","        plot_filename = 'confusion_matrix.png'\n","        plt.savefig(plot_filename)\n","        print(f\"Confusion matrix plot saved to '{plot_filename}'\")\n","        plt.show()\n","\n","        # --- Step 7: Save the Trained Model ---\n","        print(f\"\\nSaving the trained model to '{model_output_filename}'...\")\n","        model.save(model_output_filename)\n","        print(\"Model saved successfully.\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = 'chennai_water_crisis_lstm_model.h5'\n","\n","    evaluate_lstm_model(input_file, output_file)"]},{"cell_type":"code","execution_count":null,"id":"NOnPW4OjACSY","metadata":{"id":"NOnPW4OjACSY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def build_and_train_deep_lstm(input_filename, model_output_filename):\n","    \"\"\"\n","    Builds, trains, and evaluates a DEEPER LSTM model with more layers.\n","    \"\"\"\n","    try:\n","        # --- Data Loading and Preparation (Same as before) ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","        print(\"Data preparation complete.\")\n","\n","        # --- Build the DEEPER LSTM Model Architecture ---\n","        print(\"\\n--- Building the DEEPER LSTM model ---\")\n","\n","        model = Sequential()\n","\n","        # Layer 1: Input LSTM layer\n","        model.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n","        model.add(Dropout(0.2))\n","\n","        # Layer 2: Second LSTM layer\n","        model.add(LSTM(units=128, return_sequences=True))\n","        model.add(Dropout(0.2))\n","\n","        # Layer 3 (NEW): Third LSTM layer\n","        model.add(LSTM(units=64, return_sequences=True))\n","        model.add(Dropout(0.2))\n","\n","        # Layer 4: Final LSTM layer\n","        model.add(LSTM(units=64))\n","        model.add(Dropout(0.2))\n","\n","        # Layer 5 (NEW): A standard Dense layer for feature processing\n","        model.add(Dense(units=32, activation='relu'))\n","\n","        # Layer 6: Final Output Layer\n","        model.add(Dense(units=4, activation='softmax'))\n","\n","        print(\"Deeper model architecture defined.\")\n","        model.summary()\n","\n","        # --- Compile the Model ---\n","        print(\"\\nCompiling the model...\")\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        # --- Train the Model ---\n","        print(\"\\nTraining the model (this may take longer)...\")\n","        history = model.fit(\n","            X_train, y_train,\n","            epochs=60,  # Increased epochs slightly for the deeper model\n","            batch_size=32,\n","            validation_data=(X_test, y_test),\n","            verbose=1\n","        )\n","\n","        # --- Evaluate the Model ---\n","        print(\"\\n--- Evaluating Model Performance ---\")\n","        y_pred_probs = model.predict(X_test)\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"\\nModel Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for Deeper Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","        plt.savefig('confusion_matrix_deep_model.png')\n","        print(\"Confusion matrix plot for deeper model saved to 'confusion_matrix_deep_model.png'\")\n","\n","        # --- Save the Model ---\n","        model.save(model_output_filename)\n","        print(f\"\\nDeeper model saved successfully to '{model_output_filename}'\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_water_crisis_lstm_model_deep.h5'\n","\n","    build_and_train_deep_lstm(input_file, output_file)"]},{"cell_type":"code","execution_count":null,"id":"vjZfPqQkAxlW","metadata":{"id":"vjZfPqQkAxlW"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def build_and_train_complex_lstm(input_filename, model_output_filename):\n","    \"\"\"\n","    Builds, trains, and evaluates a COMPLEX \"Conv-BiLSTM\" model.\n","    \"\"\"\n","    try:\n","        # --- Data Loading and Preparation (Same as before) ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","        print(\"Data preparation complete.\")\n","\n","        # --- Build the COMPLEX Conv-BiLSTM Model Architecture ---\n","        print(\"\\n--- Building the COMPLEX Conv-BiLSTM model ---\")\n","\n","        model = Sequential()\n","\n","        # Layer 1 (NEW): 1D Convolutional Layer for pattern detection\n","        # - filters=64: It will learn 64 different patterns.\n","        # - kernel_size=5: It looks at a window of 5 days at a time.\n","        # - activation='relu': A standard activation function.\n","        model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n","\n","        # Layer 2 (NEW): Max Pooling to reduce dimensionality\n","        model.add(MaxPooling1D(pool_size=2))\n","\n","        # Layer 3 (NEW): Bidirectional LSTM layer\n","        # The Bidirectional wrapper duplicates the LSTM layer, having one process\n","        # the sequence forwards and the other backwards.\n","        model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n","        model.add(Dropout(0.3))\n","\n","        # Layer 4: Another Bidirectional LSTM layer\n","        model.add(Bidirectional(LSTM(units=128)))\n","        model.add(Dropout(0.3))\n","\n","        # Layer 5: A standard Dense layer for processing before output\n","        model.add(Dense(units=64, activation='relu'))\n","        model.add(Dropout(0.2))\n","\n","        # Layer 6: Final Output Layer\n","        model.add(Dense(units=4, activation='softmax'))\n","\n","        print(\"Complex model architecture defined.\")\n","        model.summary()\n","\n","        # --- Compile and Train ---\n","        print(\"\\nCompiling and training the complex model...\")\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        history = model.fit(\n","            X_train, y_train,\n","            epochs=60,\n","            batch_size=32,\n","            validation_data=(X_test, y_test),\n","            verbose=1\n","        )\n","\n","        # --- Evaluate the Model ---\n","        print(\"\\n--- Evaluating Model Performance ---\")\n","        y_pred_probs = model.predict(X_test)\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"\\nModel Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for Complex Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","        plt.savefig('confusion_matrix_complex_model.png')\n","        print(\"Confusion matrix plot for complex model saved to 'confusion_matrix_complex_model.png'\")\n","\n","        # --- Save the Model ---\n","        model.save(model_output_filename)\n","        print(f\"\\nComplex model saved successfully to '{model_output_filename}'\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_water_crisis_lstm_model_complex.h5'\n","\n","    build_and_train_complex_lstm(input_file, output_file)"]},{"cell_type":"code","execution_count":null,"id":"314u42zqBXsA","metadata":{"id":"314u42zqBXsA"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional\n","from tensorflow.keras.utils import to_categorical\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def build_and_evaluate_hybrid_model(input_filename):\n","    \"\"\"\n","    Builds, trains, and evaluates a hybrid model combining a Conv-BiLSTM\n","    and an XGBoost classifier.\n","    \"\"\"\n","    try:\n","        # --- Data Loading ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        # --- Data Prep for LSTM ---\n","        print(\"\\nPreparing data for LSTM...\")\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        # Also keep track of original y for XGBoost\n","        y_for_xgb = []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","            y_for_xgb.append(y.iloc[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","        y_for_xgb = np.array(y_for_xgb)\n","\n","        # Split data for both models\n","        X_train_seq, X_test_seq, y_train_cat, y_test_cat = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","        # For XGBoost, we need the original (non-sequenced, non-scaled) data\n","        X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X.iloc[sequence_length:], y_for_xgb, test_size=0.2, random_state=42)\n","\n","        # --- 1. Train the LSTM Model ---\n","        print(\"\\n--- Training Model 1: Conv-BiLSTM ---\")\n","        lstm_model = Sequential([\n","            Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n","            MaxPooling1D(pool_size=2),\n","            Bidirectional(LSTM(units=128, return_sequences=True)),\n","            Dropout(0.3),\n","            Bidirectional(LSTM(units=128)),\n","            Dropout(0.3),\n","            Dense(units=64, activation='relu'),\n","            Dropout(0.2),\n","            Dense(units=4, activation='softmax')\n","        ])\n","        lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","        lstm_model.fit(X_train_seq, y_train_cat, epochs=60, batch_size=32, validation_data=(X_test_seq, y_test_cat), verbose=0)\n","        print(\"LSTM model trained.\")\n","\n","        # --- 2. Train the XGBoost Model ---\n","        print(\"\\n--- Training Model 2: XGBoost ---\")\n","        xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=4, use_label_encoder=False, eval_metric='mlogloss')\n","        xgb_model.fit(X_train_xgb, y_train_xgb)\n","        print(\"XGBoost model trained.\")\n","\n","        # --- 3. Evaluate the Hybrid Model ---\n","        print(\"\\n--- Evaluating Hybrid Model Performance ---\")\n","\n","        # Get prediction probabilities from both models\n","        lstm_pred_probs = lstm_model.predict(X_test_seq)\n","        xgb_pred_probs = xgb_model.predict_proba(X_test_xgb)\n","\n","        # Combine predictions by averaging probabilities\n","        hybrid_pred_probs = (lstm_pred_probs + xgb_pred_probs) / 2.0\n","\n","        # Convert final probabilities to class labels\n","        hybrid_pred = np.argmax(hybrid_pred_probs, axis=1)\n","\n","        # The true labels are the non-categorical ones from the XGBoost split\n","        y_true = y_test_xgb\n","\n","        # Calculate final accuracy and confusion matrix\n","        accuracy = accuracy_score(y_true, hybrid_pred)\n","        print(f\"\\nHybrid Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        cm = confusion_matrix(y_true, hybrid_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for Hybrid (LSTM + XGBoost) Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","        plt.savefig('confusion_matrix_hybrid_model.png')\n","        print(\"Confusion matrix plot for hybrid model saved to 'confusion_matrix_hybrid_model.png'\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    build_and_evaluate_hybrid_model(input_file)"]},{"cell_type":"code","execution_count":null,"id":"h25NaNT4CFa_","metadata":{"id":"h25NaNT4CFa_"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def build_and_train_cnn_gru_hybrid(input_filename, model_output_filename):\n","    \"\"\"\n","    Builds, trains, and evaluates a hybrid model combining a CNN front-end\n","    with a GRU back-end for time-series classification.\n","    \"\"\"\n","    try:\n","        # --- Data Loading and Preparation (Same as before) ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","        print(\"Data preparation complete.\")\n","\n","        # --- Build the CNN + GRU Hybrid Model Architecture ---\n","        print(\"\\n--- Building the CNN-GRU Hybrid Model ---\")\n","\n","        model = Sequential()\n","\n","        # Part 1: The CNN Front-End\n","        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n","        model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","        model.add(MaxPooling1D(pool_size=2))\n","        model.add(Dropout(0.3))\n","\n","        # Part 2: The GRU Back-End\n","        model.add(GRU(units=100, return_sequences=True))\n","        model.add(Dropout(0.2))\n","\n","        model.add(GRU(units=100))\n","        model.add(Dropout(0.2))\n","\n","        # Part 3: The Output Layer\n","        model.add(Dense(units=4, activation='softmax'))\n","\n","        print(\"CNN-GRU Hybrid model architecture defined.\")\n","        model.summary()\n","\n","        # --- Compile and Train the Model ---\n","        print(\"\\nCompiling and training the model...\")\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        history = model.fit(\n","            X_train, y_train,\n","            epochs=60,\n","            batch_size=32,\n","            validation_data=(X_test, y_test),\n","            verbose=1\n","        )\n","\n","        # --- Evaluate the Model ---\n","        print(\"\\n--- Evaluating Model Performance ---\")\n","        y_pred_probs = model.predict(X_test)\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"\\nCNN-GRU Hybrid Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for CNN-GRU Hybrid Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","        plt.savefig('confusion_matrix_cnn_gru_hybrid.png')\n","        print(\"Confusion matrix plot for the hybrid model saved to 'confusion_matrix_cnn_gru_hybrid.png'\")\n","\n","        # --- Save the Model ---\n","        model.save(model_output_filename)\n","        print(f\"\\nCNN-GRU Hybrid model saved successfully to '{model_output_filename}'\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_water_crisis_cnn_gru_hybrid_model.h5'\n","\n","    build_and_train_cnn_gru_hybrid(input_file, output_file)"]},{"cell_type":"markdown","id":"KKQtLayfXYS0","metadata":{"id":"KKQtLayfXYS0"},"source":["### M"]},{"cell_type":"code","execution_count":null,"id":"EdIyVwETwr0Z","metadata":{"id":"EdIyVwETwr0Z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from datetime import datetime, timedelta\n","\n","def build_long_range_prediction_system(input_filename, model_output_filename):\n","    \"\"\"\n","    Builds a complete system for long-range (1-year) water crisis forecasting\n","    with prediction explanation, using an XGBoost model.\n","    \"\"\"\n","    try:\n","        # --- Step 1: Load and Prepare Clean Data ---\n","        print(\"Step 1: Loading and preparing clean data...\")\n","        df = pd.read_csv(input_filename)\n","        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n","        df = df[df['Date'] >= '2015-01-01'].copy()\n","        df.set_index('Date', inplace=True)\n","\n","        # Impute any remaining missing values robustly\n","        df.interpolate(method='linear', limit_direction='both', inplace=True)\n","        print(\"Data loaded and cleaned.\")\n","\n","        # --- Step 2: Engineer Long-Range Features ---\n","        print(\"Step 2: Engineering long-range features...\")\n","\n","        # Long-term lags\n","        for lag in [90, 180, 365]:\n","            df[f'Storage_mcft_TOTAL_lag_{lag}'] = df['Storage_mcft_TOTAL'].shift(lag)\n","            df[f'daily_average_water_level_lag_{lag}'] = df['daily_average_water_level'].shift(lag)\n","\n","        # Long-term rolling features\n","        df['rainfall_roll_sum_365'] = df['Rainfall_mm_TOTAL'].rolling(window=365).sum()\n","        df['temp_roll_mean_180'] = df['temperature_mean_celsius'].rolling(window=180).mean()\n","\n","        # Date-based features are critical for long-range forecasts\n","        df['month'] = df.index.month\n","        df['day_of_year'] = df.index.dayofyear\n","\n","        # --- Step 3: Create 1-Year Forward-Looking Target ---\n","        print(\"Step 3: Creating 1-year forward-looking target variable...\")\n","\n","        # Calculate Water Security Index (WSI)\n","        weights = {'storage': 0.4, 'groundwater': 0.3, 'rainfall': 0.15, 'consumption': 0.15}\n","        df['norm_storage'] = (df['Storage_mcft_TOTAL'] - df['Storage_mcft_TOTAL'].min()) / (df['Storage_mcft_TOTAL'].max() - df['Storage_mcft_TOTAL'].min())\n","        df['norm_groundwater'] = (df['daily_average_water_level'] - df['daily_average_water_level'].min()) / (df['daily_average_water_level'].max() - df['daily_average_water_level'].min())\n","        df['norm_rainfall'] = (df['Rainfall_mm_TOTAL'].rolling(window=30).sum() - df['Rainfall_mm_TOTAL'].rolling(window=30).sum().min()) / (df['Rainfall_mm_TOTAL'].rolling(window=30).sum().max() - df['Rainfall_mm_TOTAL'].rolling(window=30).sum().min())\n","        df['norm_consumption'] = 1 - ((df['Total_Consumption_MLD'] - df['Total_Consumption_MLD'].min()) / (df['Total_Consumption_MLD'].max() - df['Total_Consumption_MLD'].min()))\n","        df['Water_Security_Index'] = (df['norm_storage']*weights['storage'] + df['norm_groundwater']*weights['groundwater'] + df['norm_rainfall']*weights['rainfall'] + df['norm_consumption']*weights['consumption'])\n","\n","        # Shift the WSI back by 365 days to create the future target\n","        df['Future_WSI_365'] = df['Water_Security_Index'].shift(-365)\n","\n","        def define_crisis_level(score):\n","            if score < 0.35: return 3\n","            elif score < 0.55: return 2\n","            elif score < 0.70: return 1\n","            else: return 0\n","\n","        df['Crisis_Target_Future_365'] = df['Future_WSI_365'].apply(define_crisis_level)\n","\n","        # --- Step 4: Prepare Data for XGBoost ---\n","        print(\"Step 4: Finalizing data for model training...\")\n","\n","        # Drop helper columns and rows with NaNs (from lags and future shift)\n","        df_model = df.drop(columns=['Water_Security_Index', 'Future_WSI_365', 'norm_storage', 'norm_groundwater', 'norm_rainfall', 'norm_consumption'])\n","        df_model.dropna(inplace=True)\n","\n","        X = df_model.drop(columns=['Crisis_Target_Future_365'])\n","        y = df_model['Crisis_Target_Future_365']\n","\n","        # Split data chronologically for time-series validation\n","        split_index = int(len(X) * 0.8)\n","        X_train, X_test = X[:split_index], X[split_index:]\n","        y_train, y_test = y[:split_index], y[split_index:]\n","\n","        # --- Step 5: Train the XGBoost Model ---\n","        print(\"Step 5: Training XGBoost model...\")\n","        model = xgb.XGBClassifier(objective='multi:softprob', num_class=4, use_label_encoder=False, eval_metric='mlogloss', n_estimators=200, learning_rate=0.1, max_depth=5)\n","        model.fit(X_train, y_train, verbose=False)\n","\n","        # --- Step 6: Evaluate Model ---\n","        print(\"\\n--- Model Evaluation ---\")\n","        y_pred = model.predict(X_test)\n","        print(f\"Accuracy for 1-year forecast: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n","        print(\"Classification Report:\")\n","        print(classification_report(y_test, y_pred))\n","\n","        # --- Step 7: Save the Model and Feature Names ---\n","        model.save_model(model_output_filename)\n","        pd.Series(X.columns).to_csv('features.csv', index=False, header=False)\n","        print(f\"Model and feature list saved.\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","def get_prediction_and_cause(future_date_str, model_filename='long_range_model.json', features_filename='features.csv', historical_data_filename='/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'):\n","    \"\"\"\n","    Makes a prediction for a single future date and explains the cause.\n","    \"\"\"\n","    print(f\"\\n--- Generating prediction for {future_date_str} ---\")\n","\n","    # Load the trained model and historical data\n","    model = xgb.XGBClassifier()\n","    model.load_model(model_filename)\n","    features = pd.read_csv(features_filename, header=None)[0].tolist()\n","    df = pd.read_csv(historical_data_filename, parse_dates=['Date'])\n","\n","    # Get the most recent data point from history\n","    last_known_data = df.sort_values(by='Date').iloc[-1]\n","\n","    # Create a placeholder for the future date's features\n","    future_data = pd.Series(index=features)\n","    future_date = pd.to_datetime(future_date_str)\n","\n","    # Populate date-based features for the future date\n","    future_data['month'] = future_date.month\n","    future_data['day_of_year'] = future_date.dayofyear\n","\n","    # Populate other features using the last known data (this is a simplification)\n","    # A real-world system would use forecasts for weather, etc.\n","    for col in features:\n","        if col not in ['month', 'day_of_year']:\n","            # Find the corresponding column in historical data, even if it has a lag suffix\n","            base_col = col.split('_lag_')[0] if '_lag_' in col else col.split('_roll_')[0]\n","            if base_col in last_known_data:\n","                future_data[col] = last_known_data[base_col]\n","\n","    # Make the prediction\n","    prediction = model.predict(pd.DataFrame(future_data).transpose())[0]\n","    crisis_levels = ['Normal (0)', 'Alert (1)', 'Moderate Crisis (2)', 'Severe Crisis (3)']\n","    print(f\"Predicted Crisis Level: {crisis_levels[prediction]}\")\n","\n","    # Explain the cause\n","    feature_importance = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n","\n","    print(\"\\nPrimary causes for this prediction:\")\n","    for feature, importance in feature_importance.head(5).items():\n","        # Translate feature name to a more readable cause\n","        cause = feature.replace('_', ' ').title()\n","        if 'lag' in feature:\n","            cause = f\"Conditions from {feature.split('_')[-1]} days ago ({cause})\"\n","        print(f\" - {cause} (Importance: {importance:.2f})\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = 'long_range_model.json'\n","\n","    # First, train the model\n","    build_long_range_prediction_system(input_file, output_file)\n","\n","    # Now, use the trained model to make a prediction for a future date\n","    # (Example: predict for 6 months from today)\n","    future_date_to_predict = (datetime.now() + timedelta(days=180)).strftime('%Y-%m-%d')\n","    get_prediction_and_cause(future_date_to_predict)"]},{"cell_type":"code","execution_count":null,"id":"XV7Za82D3GII","metadata":{"id":"XV7Za82D3GII"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from datetime import datetime, timedelta\n","\n","def build_long_range_prediction_system(input_filename, model_output_filename):\n","    \"\"\"\n","    Builds a complete system for long-range (1-year) water crisis forecasting\n","    with prediction explanation, using an XGBoost model.\n","    \"\"\"\n","    try:\n","        # --- Step 1: Load and Prepare Clean Data ---\n","        print(\"Step 1: Loading and preparing clean data...\")\n","        df = pd.read_csv(input_filename)\n","        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n","        df = df[df['Date'] >= '2015-01-01'].copy()\n","        df.set_index('Date', inplace=True)\n","\n","        # Impute any remaining missing values robustly\n","        df.interpolate(method='linear', limit_direction='both', inplace=True)\n","        print(\"Data loaded and cleaned.\")\n","\n","        # --- Step 2: Engineer Long-Range Features ---\n","        print(\"Step 2: Engineering long-range features...\")\n","\n","        # Long-term lags\n","        for lag in [90, 180, 365]:\n","            df[f'Storage_mcft_TOTAL_lag_{lag}'] = df['Storage_mcft_TOTAL'].shift(lag)\n","            df[f'daily_average_water_level_lag_{lag}'] = df['daily_average_water_level'].shift(lag)\n","\n","        # Long-term rolling features\n","        df['rainfall_roll_sum_365'] = df['Rainfall_mm_TOTAL'].rolling(window=365).sum()\n","        df['temp_roll_mean_180'] = df['temperature_mean_celsius'].rolling(window=180).mean()\n","\n","        # Date-based features are critical for long-range forecasts\n","        df['month'] = df.index.month\n","        df['day_of_year'] = df.index.dayofyear\n","\n","        # --- Step 3: Create 1-Year Forward-Looking Target ---\n","        print(\"Step 3: Creating 1-year forward-looking target variable...\")\n","\n","        # Calculate Water Security Index (WSI)\n","        weights = {'storage': 0.4, 'groundwater': 0.3, 'rainfall': 0.15, 'consumption': 0.15}\n","        df['norm_storage'] = (df['Storage_mcft_TOTAL'] - df['Storage_mcft_TOTAL'].min()) / (df['Storage_mcft_TOTAL'].max() - df['Storage_mcft_TOTAL'].min())\n","        df['norm_groundwater'] = (df['daily_average_water_level'] - df['daily_average_water_level'].min()) / (df['daily_average_water_level'].max() - df['daily_average_water_level'].min())\n","        df['norm_rainfall'] = (df['Rainfall_mm_TOTAL'].rolling(window=30).sum() - df['Rainfall_mm_TOTAL'].rolling(window=30).sum().min()) / (df['Rainfall_mm_TOTAL'].rolling(window=30).sum().max() - df['Rainfall_mm_TOTAL'].rolling(window=30).sum().min())\n","        df['norm_consumption'] = 1 - ((df['Total_Consumption_MLD'] - df['Total_Consumption_MLD'].min()) / (df['Total_Consumption_MLD'].max() - df['Total_Consumption_MLD'].min()))\n","        df['Water_Security_Index'] = (df['norm_storage']*weights['storage'] + df['norm_groundwater']*weights['groundwater'] + df['norm_rainfall']*weights['rainfall'] + df['norm_consumption']*weights['consumption'])\n","\n","        # Shift the WSI back by 365 days to create the future target\n","        df['Future_WSI_365'] = df['Water_Security_Index'].shift(-365)\n","\n","        def define_crisis_level(score):\n","            if score < 0.35: return 3\n","            elif score < 0.55: return 2\n","            elif score < 0.70: return 1\n","            else: return 0\n","\n","        df['Crisis_Target_Future_365'] = df['Future_WSI_365'].apply(define_crisis_level)\n","\n","        # --- Step 4: Prepare Data for XGBoost ---\n","        # --- Step 4: Prepare Data with STRATIFIED Split ---\n","        print(\"Step 4: Finalizing data with a STRATIFIED train-test split...\")\n","\n","        df_model = df.drop(columns=['Water_Security_Index', 'Future_WSI_365', 'norm_storage', 'norm_groundwater', 'norm_rainfall', 'norm_consumption'])\n","        df_model.dropna(inplace=True)\n","\n","        X = df_model.drop(columns=['Crisis_Target_Future_365'])\n","        y = df_model['Crisis_Target_Future_365']\n","\n","        # --- THIS IS THE KEY CHANGE ---\n","        # We now use train_test_split with the `stratify` parameter.\n","        # This ensures both train and test sets have a representative sample of all crisis levels.\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X, y, test_size=0.2, random_state=42, stratify=y\n","        )\n","\n","        # --- Step 5: Train the XGBoost Model ---\n","        print(\"Step 5: Training XGBoost model...\")\n","        model = xgb.XGBClassifier(objective='multi:softprob', num_class=4, use_label_encoder=False, eval_metric='mlogloss', n_estimators=200, learning_rate=0.1, max_depth=5)\n","        model.fit(X_train, y_train, verbose=False)\n","\n","        # --- Step 6: Evaluate Model ---\n","        print(\"\\n--- Model Evaluation ---\")\n","        y_pred = model.predict(X_test)\n","        print(f\"Accuracy for 1-year forecast: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n","        print(\"Classification Report:\")\n","        print(classification_report(y_test, y_pred))\n","\n","        # --- Step 7: Save the Model and Feature Names ---\n","        model.save_model(model_output_filename)\n","        pd.Series(X.columns).to_csv('features.csv', index=False, header=False)\n","        print(f\"Model and feature list saved.\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","def get_prediction_and_cause(future_date_str, model_filename='long_range_model.json', features_filename='features.csv', historical_data_filename='/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'):\n","    \"\"\"\n","    Makes a prediction for a single future date and explains the cause.\n","    \"\"\"\n","    print(f\"\\n--- Generating prediction for {future_date_str} ---\")\n","\n","    # Load the trained model and historical data\n","    model = xgb.XGBClassifier()\n","    model.load_model(model_filename)\n","    features = pd.read_csv(features_filename, header=None)[0].tolist()\n","    df = pd.read_csv(historical_data_filename, parse_dates=['Date'])\n","\n","    # Get the most recent data point from history\n","    last_known_data = df.sort_values(by='Date').iloc[-1]\n","\n","    # Create a placeholder for the future date's features\n","    future_data = pd.Series(index=features)\n","    future_date = pd.to_datetime(future_date_str)\n","\n","    # Populate date-based features for the future date\n","    future_data['month'] = future_date.month\n","    future_data['day_of_year'] = future_date.dayofyear\n","\n","    # Populate other features using the last known data (this is a simplification)\n","    # A real-world system would use forecasts for weather, etc.\n","    for col in features:\n","        if col not in ['month', 'day_of_year']:\n","            # Find the corresponding column in historical data, even if it has a lag suffix\n","            base_col = col.split('_lag_')[0] if '_lag_' in col else col.split('_roll_')[0]\n","            if base_col in last_known_data:\n","                future_data[col] = last_known_data[base_col]\n","\n","    # Make the prediction\n","    prediction = model.predict(pd.DataFrame(future_data).transpose())[0]\n","    crisis_levels = ['Normal (0)', 'Alert (1)', 'Moderate Crisis (2)', 'Severe Crisis (3)']\n","    print(f\"Predicted Crisis Level: {crisis_levels[prediction]}\")\n","\n","    # Explain the cause\n","    feature_importance = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n","\n","    print(\"\\nPrimary causes for this prediction:\")\n","    for feature, importance in feature_importance.head(5).items():\n","        # Translate feature name to a more readable cause\n","        cause = feature.replace('_', ' ').title()\n","        if 'lag' in feature:\n","            cause = f\"Conditions from {feature.split('_')[-1]} days ago ({cause})\"\n","        print(f\" - {cause} (Importance: {importance:.2f})\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = 'long_range_model.json'\n","\n","    # First, train the model\n","    build_long_range_prediction_system(input_file, output_file)\n","\n","    # Now, use the trained model to make a prediction for a future date\n","    # (Example: predict for 6 months from today)\n","    future_date_to_predict = (datetime.now() + timedelta(days=180)).strftime('%Y-%m-%d')\n","    get_prediction_and_cause(future_date_to_predict)"]},{"cell_type":"code","execution_count":null,"id":"SuaLjWtu3eM-","metadata":{"id":"SuaLjWtu3eM-"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from datetime import datetime, timedelta\n","\n","def build_realistic_long_range_model(input_filename, model_output_filename):\n","    \"\"\"\n","    Builds a more realistic long-range forecasting model by removing features\n","    that could cause subtle target leakage.\n","    \"\"\"\n","    try:\n","        # Step 1: Load and Prepare Data (Same as before)\n","        print(\"Step 1: Loading and preparing clean data...\")\n","        df = pd.read_csv(input_filename)\n","        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n","        df = df[df['Date'] >= '2015-01-01'].copy()\n","\n","        # --- FIX: Remove the \"leaky\" annual features at the beginning ---\n","        cols_to_remove = [col for col in df.columns if 'Storage_as_on_same_day_last_year' in col]\n","        df = df.drop(columns=cols_to_remove)\n","        print(f\"Removed {len(cols_to_remove)} potentially leaky 'last year' feature columns.\")\n","\n","        df.set_index('Date', inplace=True)\n","        df.interpolate(method='linear', limit_direction='both', inplace=True)\n","\n","        # Step 2: Engineer Long-Range Features (without 365-day lags)\n","        print(\"Step 2: Engineering long-range features...\")\n","\n","        # Shorter-term lags are okay\n","        for lag in [90, 180]:\n","            df[f'Storage_mcft_TOTAL_lag_{lag}'] = df['Storage_mcft_TOTAL'].shift(lag)\n","            df[f'daily_average_water_level_lag_{lag}'] = df['daily_average_water_level'].shift(lag)\n","\n","        df['rainfall_roll_sum_180'] = df['Rainfall_mm_TOTAL'].rolling(window=180).sum()\n","        df['temp_roll_mean_180'] = df['temperature_mean_celsius'].rolling(window=180).mean()\n","\n","        df['month'] = df.index.month\n","        df['day_of_year'] = df.index.dayofyear\n","\n","        # Step 3: Create 1-Year Forward-Looking Target (Same as before)\n","        print(\"Step 3: Creating 1-year forward-looking target...\")\n","        weights = {'storage': 0.4, 'groundwater': 0.3, 'rainfall': 0.15, 'consumption': 0.15}\n","        df['norm_storage'] = (df['Storage_mcft_TOTAL'] - df['Storage_mcft_TOTAL'].min()) / (df['Storage_mcft_TOTAL'].max() - df['Storage_mcft_TOTAL'].min())\n","        df['norm_groundwater'] = (df['daily_average_water_level'] - df['daily_average_water_level'].min()) / (df['daily_average_water_level'].max() - df['daily_average_water_level'].min())\n","        df['norm_rainfall'] = (df['Rainfall_mm_TOTAL'].rolling(window=30).sum() - df['Rainfall_mm_TOTAL'].rolling(window=30).sum().min()) / (df['Rainfall_mm_TOTAL'].rolling(window=30).sum().max() - df['Rainfall_mm_TOTAL'].rolling(window=30).sum().min())\n","        df['norm_consumption'] = 1 - ((df['Total_Consumption_MLD'] - df['Total_Consumption_MLD'].min()) / (df['Total_Consumption_MLD'].max() - df['Total_Consumption_MLD'].min()))\n","        df['Water_Security_Index'] = (df['norm_storage']*weights['storage'] + df['norm_groundwater']*weights['groundwater'] + df['norm_rainfall']*weights['rainfall'] + df['norm_consumption']*weights['consumption'])\n","        df['Future_WSI_365'] = df['Water_Security_Index'].shift(-365)\n","        def define_crisis_level(score):\n","            if score < 0.35: return 3\n","            elif score < 0.55: return 2\n","            elif score < 0.70: return 1\n","            else: return 0\n","        df['Crisis_Target_Future_365'] = df['Future_WSI_365'].apply(define_crisis_level)\n","\n","        # Step 4: Prepare Data with Stratified Split (Same as before)\n","        print(\"Step 4: Finalizing data with a stratified train-test split...\")\n","        df_model = df.drop(columns=['Water_Security_Index', 'Future_WSI_365', 'norm_storage', 'norm_groundwater', 'norm_rainfall', 'norm_consumption'])\n","        df_model.dropna(inplace=True)\n","\n","        X = df_model.drop(columns=['Crisis_Target_Future_365'])\n","        y = df_model['Crisis_Target_Future_365']\n","\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X, y, test_size=0.2, random_state=42, stratify=y\n","        )\n","\n","        # Step 5 & 6: Train and Evaluate the Model\n","        print(\"Step 5: Training XGBoost model...\")\n","        model = xgb.XGBClassifier(objective='multi:softprob', num_class=4, use_label_encoder=False, eval_metric='mlogloss', n_estimators=200, learning_rate=0.1, max_depth=5)\n","        model.fit(X_train, y_train, verbose=False)\n","\n","        print(\"\\n--- Model Evaluation (Realistic) ---\")\n","        y_pred = model.predict(X_test)\n","        print(f\"Realistic Accuracy for 1-year forecast: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n","        print(\"Classification Report:\")\n","        print(classification_report(y_test, y_pred))\n","         # --- FIX: ADDED THE MISSING SAVE COMMANDS ---\n","        print(\"\\nStep 7: Saving the Model and Feature List...\")\n","        model.save_model(model_output_filename)\n","        pd.Series(X.columns).to_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/features.csv', index=False, header=False)\n","        print(f\"Model saved to '{model_output_filename}' and feature list saved to 'features.csv'.\")\n","        # ----------------------------------------------\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/long_range_model_realistic.json'\n","    build_realistic_long_range_model(input_file, output_file)"]},{"cell_type":"code","execution_count":null,"id":"MjaYRvz4lf8Z","metadata":{"id":"MjaYRvz4lf8Z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","def build_train_save_model_and_scaler(input_filename, model_output_filename, scaler_output_filename):\n","    \"\"\"\n","    Loads data, builds and trains the LSTM with Attention model,\n","    then saves both the trained model and the scaler object.\n","    \"\"\"\n","    try:\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","        joblib.dump(scaler, scaler_output_filename)\n","        print(f\"Scaler object saved to '{scaler_output_filename}'\")\n","\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","        X_sequences, y_sequences = np.array(X_sequences), np.array(y_sequences)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X_sequences,\n","            y_sequences,\n","            test_size=0.2,\n","            random_state=42,\n","            stratify=y_sequences\n","        )\n","        print(\"Data preparation complete with a STRATIFIED split.\")\n","\n","        print(\"\\nBuilding the LSTM with Attention model...\")\n","        input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n","        lstm_out_1 = LSTM(units=128, return_sequences=True)(input_layer)\n","        lstm_out_2 = LSTM(units=128, return_sequences=True)(lstm_out_1)\n","\n","        attention_out = Attention()([lstm_out_2, lstm_out_2])\n","\n","        lstm_out_3 = LSTM(units=64)(attention_out)\n","        dropout_1 = Dropout(0.3)(lstm_out_3)\n","        dense_1 = Dense(units=64, activation='relu')(dropout_1)\n","        output_layer = Dense(units=4, activation='softmax')(dense_1)\n","\n","        model = Model(inputs=input_layer, outputs=output_layer)\n","\n","        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        history = model.fit(\n","            X_train, y_train,\n","            epochs=100,\n","            batch_size=32,\n","            validation_data=(X_test, y_test),\n","            callbacks=[early_stopping],\n","            verbose=1\n","        )\n","\n","        model.save(model_output_filename)\n","        print(f\"\\nModel saved successfully to '{model_output_filename}'\")\n","\n","        # Evaluate and plot confusion matrix after training\n","        print(\"\\n--- Evaluating Model Performance ---\")\n","        y_pred_probs = model.predict(X_test)\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"LSTM with Attention Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for LSTM with Attention Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","        plt.savefig('confusion_matrix_attention_model.png')\n","        print(\"Confusion matrix plot saved to 'confusion_matrix_attention_model.png'\")\n","        plt.show()\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","def load_and_evaluate_model(input_filename, model_filename, scaler_filename):\n","    \"\"\"\n","    Loads a saved model and scaler, prepares the data, and evaluates model performance\n","    by printing accuracy, classification report, and plotting the confusion matrix.\n","    \"\"\"\n","    try:\n","        print(f\"Loading model from '{model_filename}'...\")\n","        model = tf.keras.models.load_model(model_filename)\n","\n","        print(f\"Loading scaler from '{scaler_filename}'...\")\n","        scaler = joblib.load(scaler_filename)\n","\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        print(\"\\nPreparing test data...\")\n","        X_scaled = scaler.transform(X)\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        _, X_test, _, y_test = train_test_split(\n","            X_sequences,\n","            y_sequences,\n","            test_size=0.2,\n","            random_state=42,\n","            stratify=y_sequences\n","        )\n","\n","        print(f\"Test data prepared with {len(X_test)} samples.\")\n","\n","        print(\"\\n--- Model Performance Evaluation ---\")\n","        y_pred_probs = model.predict(X_test)\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"Overall Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(y_true, y_pred, target_names=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)']))\n","\n","        print(\"--- Generating Confusion Matrix ---\")\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for LSTM with Attention Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","\n","        plot_filename = 'confusion_matrix_evaluation.png'\n","        plt.savefig(plot_filename)\n","        print(f\"Confusion matrix plot saved to '{plot_filename}'\")\n","        plt.show()\n","\n","    except FileNotFoundError as e:\n","        print(f\"Error: A required file was not found. Missing file: {e.filename}\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_model_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/chennai_water_crisis_attention_model.h5'\n","    output_scaler_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/scaler.joblib'\n","\n","    # Train the model and save it along with the scaler\n","    build_train_save_model_and_scaler(input_file, output_model_file, output_scaler_file)\n","\n","    # Load the saved model and scaler, then evaluate performance\n","    load_and_evaluate_model(input_file, output_model_file, output_scaler_file)\n"]},{"cell_type":"code","execution_count":null,"id":"OI7XsmdE_DO3","metadata":{"id":"OI7XsmdE_DO3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    \"\"\"Creates a single block of a Transformer encoder.\"\"\"\n","    # Attention and Normalization\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(x, x)\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(dropout)(x)\n","    x = Dense(inputs.shape[-1])(x)\n","    return x + res\n","\n","def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, num_classes):\n","    \"\"\"Builds a Transformer model for time-series classification.\"\"\"\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim)\n","\n","    # Global Average Pooling takes the average over the sequence dimension.\n","    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","\n","    # Final dense layers for classification\n","    x = Dropout(0.4)(x)\n","    x = Dense(128, activation=\"relu\")(x)\n","    x = Dropout(0.4)(x)\n","    outputs = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    return Model(inputs, outputs)\n","\n","def run_transformer_experiment(input_filename, model_output_filename):\n","    \"\"\"\n","    Main function to load data, build, train, and evaluate the Transformer model.\n","    \"\"\"\n","    try:\n","        # --- Data Loading and Preparation (Same as before) ---\n","        print(f\"Loading data from '{input_filename}'...\")\n","        df = pd.read_csv(input_filename)\n","        df = df.drop(columns=['Date'])\n","\n","        X = df.drop(columns=['Crisis_Target']).astype('float32')\n","        y = df['Crisis_Target'].astype('int')\n","\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        X_scaled = scaler.fit_transform(X)\n","\n","        y_categorical = to_categorical(y, num_classes=4)\n","\n","        sequence_length = 60\n","        X_sequences, y_sequences = [], []\n","        for i in range(len(X_scaled) - sequence_length):\n","            X_sequences.append(X_scaled[i:i + sequence_length])\n","            y_sequences.append(y_categorical[i + sequence_length])\n","\n","        X_sequences = np.array(X_sequences)\n","        y_sequences = np.array(y_sequences)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n","        print(\"Data preparation complete.\")\n","\n","        # --- Build the Transformer Model ---\n","        print(\"\\n--- Building the Transformer Model ---\")\n","        model = build_transformer_model(\n","            input_shape=X_train.shape[1:],\n","            head_size=256,\n","            num_heads=4,\n","            ff_dim=4,\n","            num_transformer_blocks=4,\n","            num_classes=4\n","        )\n","\n","        model.summary()\n","\n","        # --- Compile and Train ---\n","        print(\"\\nCompiling and training the Transformer model...\")\n","        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        # Add EarlyStopping for robust training\n","        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        history = model.fit(\n","            X_train, y_train,\n","            epochs=100,\n","            batch_size=32,\n","            validation_data=(X_test, y_test),\n","            callbacks=[early_stopping],\n","            verbose=1\n","        )\n","\n","        # --- Evaluate ---\n","        print(\"\\n--- Evaluating Model Performance ---\")\n","        y_pred_probs = model.predict(X_test)\n","        y_pred = np.argmax(y_pred_probs, axis=1)\n","        y_true = np.argmax(y_test, axis=1)\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        print(f\"\\nTransformer Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n","\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',\n","                    xticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'],\n","                    yticklabels=['Normal (0)', 'Alert (1)', 'Moderate (2)', 'Severe (3)'])\n","        plt.title('Confusion Matrix for Transformer Model')\n","        plt.ylabel('Actual Crisis Level')\n","        plt.xlabel('Predicted Crisis Level')\n","        plt.savefig('confusion_matrix_transformer_model.png')\n","        print(\"Confusion matrix plot for the Transformer model saved to 'confusion_matrix_transformer_model.png'\")\n","\n","        # --- Save ---\n","        model.save(model_output_filename)\n","        print(f\"\\nTransformer model saved successfully to '{model_output_filename}'\")\n","\n","    except FileNotFoundError:\n","        print(f\"Error: The input file '{input_filename}' was not found.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main execution ---\n","if __name__ == '__main__':\n","    input_file = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","    output_file = 'chennai_water_crisis_transformer_model.keras' # Using the new .keras format\n","\n","    run_transformer_experiment(input_file, output_file)"]},{"cell_type":"markdown","id":"qRZu2Pz8itEa","metadata":{"id":"qRZu2Pz8itEa"},"source":["## testing the saved model"]},{"cell_type":"code","execution_count":null,"id":"VA-iP18uFQQ_","metadata":{"id":"VA-iP18uFQQ_"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from datetime import datetime, timedelta\n","\n","def prepare_features_for_date(target_date, historical_df, feature_list):\n","    \"\"\"\n","    Prepares a single row of features for a specific target date using historical data.\n","    This is a crucial step to prevent data leakage during live prediction.\n","    \"\"\"\n","    # Create a placeholder for the features we need to build\n","    features = pd.Series(index=feature_list)\n","\n","    # --- Step 1: Create Date-Based Features ---\n","    features['month'] = target_date.month\n","    features['day_of_year'] = target_date.dayofyear\n","\n","    # --- Step 2: Create Lag Features ---\n","    # For each lag, we find the data from the correct point in the past.\n","    for lag in [90, 180]:\n","        lag_date = target_date - pd.Timedelta(days=lag)\n","        # Find the closest available date in the history\n","        lag_data = historical_df[historical_df['Date'] <= lag_date].iloc[-1]\n","        features[f'Storage_mcft_TOTAL_lag_{lag}'] = lag_data['Storage_mcft_TOTAL']\n","        features[f'daily_average_water_level_lag_{lag}'] = lag_data['daily_average_water_level']\n","\n","    # --- Step 3: Create Rolling Features ---\n","    # For rolling stats, we calculate them over the period ending *before* the target date.\n","    end_date = target_date - pd.Timedelta(days=1)\n","    start_date_180 = end_date - pd.Timedelta(days=179)\n","\n","    rolling_period_df = historical_df[(historical_df['Date'] >= start_date_180) & (historical_df['Date'] <= end_date)]\n","    features['rainfall_roll_sum_180'] = rolling_period_df['Rainfall_mm_TOTAL'].sum()\n","    features['temp_roll_mean_180'] = rolling_period_df['temperature_mean_celsius'].mean()\n","\n","    # --- Step 4: Fill any other direct features using the last known data ---\n","    # This assumes direct features remain constant from the last known day.\n","    last_known_data = historical_df[historical_df['Date'] < target_date].iloc[-1]\n","    direct_features = [col for col in feature_list if '_lag_' not in col and '_roll_' not in col and 'month' not in col and 'day_of_year' not in col]\n","    for col in direct_features:\n","         if col in last_known_data:\n","            features[col] = last_known_data[col]\n","\n","    # Return the completed feature set, ensuring it's in the correct order\n","    return pd.DataFrame(features).transpose()[feature_list]\n","\n","\n","def predict_and_explain(date_str):\n","    \"\"\"\n","    Loads the XGBoost model and data, then provides a prediction and analysis for a given date.\n","    \"\"\"\n","    try:\n","        # --- Load Model and Data ---\n","        model = xgb.XGBClassifier()\n","        model.load_model('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/long_range_model_realistic.json')\n","\n","        feature_list = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/features.csv', header=None)[0].tolist()\n","\n","        historical_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv', parse_dates=['Date'])\n","\n","        # --- Process Input ---\n","        target_date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","        print(f\"\\n--- Generating 1-Year Forecast for Target Date: {target_date.date()} ---\")\n","\n","        # The prediction is for 365 days after the \"current\" data.\n","        # So, the features need to be prepared for the date one year prior.\n","        feature_date = target_date - pd.Timedelta(days=365)\n","        print(f\"(Using historical data up to {feature_date.date()} to make the prediction)\")\n","\n","        # --- Prepare Features for the Target Date ---\n","        input_features = prepare_features_for_date(feature_date, historical_df, feature_list)\n","\n","        # --- Make Prediction ---\n","        pred_probs = model.predict_proba(input_features)[0]\n","        prediction = np.argmax(pred_probs)\n","        confidence = pred_probs[prediction]\n","\n","        crisis_levels = ['Normal (0)', 'Alert (1)', 'Moderate Crisis (2)', 'Severe Crisis (3)']\n","\n","        # --- Explain the Cause ---\n","        feature_importance = pd.Series(model.feature_importances_, index=feature_list).sort_values(ascending=False)\n","        total_importance = feature_importance.sum()\n","\n","        print(f\"\\nModel Prediction: {crisis_levels[prediction]}\")\n","\n","        # --- Different Output Based on Year ---\n","        if target_date.year == 2024:\n","            # We can find the actual result for comparison\n","            actual_row = historical_df[historical_df['Date'] == target_date]\n","            if not actual_row.empty:\n","                print(\"\\nThis is a validation prediction on data the model has not seen.\")\n","                # You would need to re-calculate the true crisis target for this date to compare\n","                # For now, we will just state that the actuals are available.\n","                print(\"Actual historical data for this date is available for comparison.\")\n","            else:\n","                print(\"No historical data found for this date in 2024 to compare against.\")\n","\n","        else: # For future dates or past dates not in 2024\n","            print(f\"Confidence: {confidence:.2%}\")\n","\n","        print(\"\\nTop 5 Causes for this Prediction:\")\n","        for feature, importance in feature_importance.head(5).items():\n","            cause = feature.replace('_', ' ').title()\n","            percentage = (importance / total_importance) * 100\n","            print(f\" - {cause} ({percentage:.1f}%)\")\n","\n","    except FileNotFoundError as e:\n","        print(f\"Error: A required file was not found. Make sure the model and data files are in the same directory.\")\n","        print(e)\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        print(\"Please ensure the date is in YYYY-MM-DD format and is far enough in the future (at least 1.5 years after the last data point).\")\n","\n","# --- Main Execution: Examples ---\n","if __name__ == '__main__':\n","    # Example 1: A validation check for a date in 2024 (using data from 2023 to predict)\n","    predict_and_explain('2024-11-15')\n","\n","    # Example 2: A true future forecast (using the latest data to predict one year ahead)\n","    future_date = (datetime.now() + timedelta(days=365)).strftime('%Y-%m-%d')\n","    predict_and_explain(future_date)\n","\n","    # Example 3: User can input their own date\n","    print(\"\\n-------------------------------------------------\")\n","    user_date = input(\"Enter a future date (YYYY-MM-DD) to predict: \")\n","    if user_date:\n","        predict_and_explain(user_date)"]},{"cell_type":"markdown","id":"9czZGu_finNx","metadata":{"id":"9czZGu_finNx"},"source":["## testing the saved model\n"]},{"cell_type":"code","execution_count":null,"id":"YfLRpw4AnI2z","metadata":{"id":"YfLRpw4AnI2z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from datetime import datetime, timedelta\n","import shap # <-- Make sure shap is installed\n","\n","def prepare_features_for_date(target_date, historical_df, feature_list):\n","    \"\"\"\n","    Prepares a single row of features for a specific target date using historical data.\n","    \"\"\"\n","    features = pd.Series(index=feature_list)\n","\n","    # Create Date-Based Features\n","    features['month'] = target_date.month\n","    features['day_of_year'] = target_date.dayofyear\n","\n","    # Create Lag Features\n","    for lag in [90, 180]:\n","        lag_date = target_date - pd.Timedelta(days=lag)\n","        lag_data = historical_df[historical_df['Date'] <= lag_date].iloc[-1]\n","        features[f'Storage_mcft_TOTAL_lag_{lag}'] = lag_data['Storage_mcft_TOTAL']\n","        features[f'daily_average_water_level_lag_{lag}'] = lag_data['daily_average_water_level']\n","\n","    # Create Rolling Features\n","    end_date = target_date - pd.Timedelta(days=1)\n","    start_date_180 = end_date - pd.Timedelta(days=179)\n","    rolling_period_df = historical_df[(historical_df['Date'] >= start_date_180) & (historical_df['Date'] <= end_date)]\n","    features['rainfall_roll_sum_180'] = rolling_period_df['Rainfall_mm_TOTAL'].sum()\n","    features['temp_roll_mean_180'] = rolling_period_df['temperature_mean_celsius'].mean()\n","\n","    # Fill any other direct features using the last known data\n","    last_known_data = historical_df[historical_df['Date'] < target_date].iloc[-1]\n","    direct_features = [col for col in feature_list if '_lag_' not in col and '_roll_' not in col and 'month' not in col and 'day_of_year' not in col]\n","    for col in direct_features:\n","         if col in last_known_data:\n","            features[col] = last_known_data[col]\n","\n","    return pd.DataFrame(features).transpose()[feature_list]\n","\n","\n","def predict_and_explain_with_shap(date_str):\n","    \"\"\"\n","    Loads the XGBoost model, provides a prediction, and uses SHAP to explain\n","    the causes for that specific prediction, handling both old and new SHAP formats.\n","    \"\"\"\n","    try:\n","        # --- Load Model and Data ---\n","        model = xgb.XGBClassifier()\n","        model.load_model('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/long_range_model_realistic.json')\n","\n","        feature_list = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/features.csv', header=None)[0].tolist()\n","        historical_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv', parse_dates=['Date'])\n","\n","        # --- Process Input and Prepare Features ---\n","        target_date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","        print(f\"\\n--- Generating 1-Year Forecast for Target Date: {target_date.date()} ---\")\n","\n","        feature_date = target_date - pd.Timedelta(days=365)\n","        print(f\"(Using historical data up to {feature_date.date()} to make the prediction)\")\n","\n","        input_features = prepare_features_for_date(feature_date, historical_df, feature_list)\n","\n","        # --- Make Prediction ---\n","        pred_probs = model.predict_proba(input_features)[0]\n","        prediction = np.argmax(pred_probs)\n","        confidence = pred_probs[prediction]\n","\n","        crisis_levels = ['Normal (0)', 'Alert (1)', 'Moderate Crisis (2)', 'Severe Crisis (3)']\n","\n","        print(f\"\\nModel Prediction: {crisis_levels[prediction]}\")\n","        print(f\"Confidence: {confidence:.2%}\")\n","\n","        explainer = shap.TreeExplainer(model)\n","        shap_values = explainer.shap_values(input_features)\n","\n","        print(\"Type of shap_values:\", type(shap_values))\n","        print(\"Shape of shap_values ndarray:\", shap_values.shape)\n","\n","        # Extract SHAP values based on the shape you have (samples, features, classes)\n","        if isinstance(shap_values, np.ndarray) and shap_values.shape[0] == 1 and shap_values.shape[1] == len(feature_list):\n","            shap_values_for_prediction = shap_values[0, :, prediction]  # (96,)\n","        else:\n","            raise ValueError(f\"Unexpected SHAP output format or shape: {type(shap_values)}, {np.shape(shap_values)}\")\n","\n","        shap_series = pd.Series(shap_values_for_prediction, index=feature_list)\n","\n","        # Sort and get top 5 features by absolute SHAP value\n","        top_5_features = shap_series.abs().sort_values(ascending=False).head(5)\n","\n","        print(\"\\nTop 5 Causes for this SPECIFIC Prediction:\")\n","        for feature_name in top_5_features.index:\n","            shap_value = shap_series[feature_name]\n","            feature = feature_name.replace('_', ' ').title()\n","\n","            # Make human-readable: absolute value  percentage with 2 decimals\n","            impact_score = abs(shap_value) * 100\n","\n","            print(f\" - {feature}: Impact Score {impact_score:.2f}%\")\n","    except FileNotFoundError as e:\n","        print(f\"Error: A required file was not found. {e}\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","# --- Main Execution: Examples ---\n","if __name__ == '__main__':\n","    predict_and_explain_with_shap('2024-11-15')\n","    predict_and_explain_with_shap('2026-03-09')"]},{"cell_type":"markdown","id":"sG4Uml1TNM0y","metadata":{"id":"sG4Uml1TNM0y"},"source":["XBG testing from saved model"]},{"cell_type":"code","execution_count":null,"id":"arGJ6d2OmDTR","metadata":{"id":"arGJ6d2OmDTR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import joblib\n","import shap\n","\n","def predict_with_attention_model(date_str):\n","    \"\"\"\n","    Loads the saved LSTM w/ Attention model and scaler to make a prediction\n","    for a specific date and explain the causes using SHAP.\n","    \"\"\"\n","    try:\n","        # --- Load all necessary files ---\n","        model_path = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/chennai_water_crisis_attention_model.h5'\n","        scaler_path = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Models/scaler.joblib'\n","        data_path = '/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv'\n","\n","        print(\"Loading model, scaler, and historical data...\")\n","        # Load the model with custom_objects for the Attention layer if needed\n","        model = tf.keras.models.load_model(model_path)\n","        scaler = joblib.load(scaler_path)\n","        historical_df = pd.read_csv(data_path, parse_dates=['Date'])\n","\n","        target_date = pd.to_datetime(date_str, format='%Y-%m-%d')\n","        print(f\"\\n--- Generating Prediction for: {target_date.date()} ---\")\n","\n","        # --- Prepare the input sequence ---\n","        sequence_length = 60\n","        # Get the 60 days of data immediately preceding the target date\n","        input_df = historical_df[historical_df['Date'] < target_date].tail(sequence_length)\n","\n","        if len(input_df) < sequence_length:\n","            print(f\"Error: Not enough historical data to make a prediction. Need {sequence_length} days, but found {len(input_df)}.\")\n","            return\n","\n","        features = input_df.drop(columns=['Date', 'Crisis_Target']).astype('float32')\n","\n","        # Scale the features using the loaded scaler\n","        scaled_features = scaler.transform(features)\n","\n","        # Reshape for the LSTM model\n","        input_sequence = np.reshape(scaled_features, (1, sequence_length, features.shape[1]))\n","\n","        # --- Make Prediction ---\n","        pred_probs = model.predict(input_sequence)[0]\n","        prediction = np.argmax(pred_probs)\n","        confidence = pred_probs[prediction]\n","        crisis_levels = ['Normal (0)', 'Alert (1)', 'Moderate Crisis (2)', 'Severe Crisis (3)']\n","\n","        print(f\"\\nModel Prediction: {crisis_levels[prediction]}\")\n","\n","        # --- Handle different output based on the year ---\n","        if target_date.year == 2024 and target_date in historical_df['Date'].values:\n","            actual_value = historical_df[historical_df['Date'] == target_date]['Crisis_Target'].iloc[0]\n","            print(f\"Actual Result: {crisis_levels[actual_value]}\")\n","            print(\"\\nThis is a validation prediction on data the model has not seen during training.\")\n","        else:\n","            print(f\"Confidence: {confidence:.2%}\")\n","\n","        # --- Explain the Cause with SHAP ---\n","        print(\"\\nTop 5 Causes for this SPECIFIC Prediction:\")\n","        # SHAP for deep models requires a background dataset to compare against.\n","        # We'll use a sample of the training data as the background.\n","        background_sample = historical_df.sample(100).drop(columns=['Date', 'Crisis_Target']).astype('float32')\n","        background_scaled = scaler.transform(background_sample)\n","\n","        # Reshape background for the sequence model\n","        background_sequences = []\n","        for i in range(len(background_scaled) - sequence_length):\n","             background_sequences.append(background_scaled[i:i+sequence_length])\n","\n","        explainer = shap.DeepExplainer(model, np.array(background_sequences))\n","        shap_values = explainer.shap_values(input_sequence)\n","\n","        # Get SHAP values for the predicted class and average them over the time steps\n","        shap_values_for_prediction = np.abs(shap_values[prediction][0]).mean(axis=0)\n","\n","        # Create a DataFrame for analysis\n","        shap_df = pd.DataFrame({\n","            'feature': features.columns,\n","            'shap_value': shap_values_for_prediction\n","        }).sort_values(by='shap_value', ascending=False)\n","\n","        for index, row in shap_df.head(5).iterrows():\n","            feature = row['feature'].replace('_', ' ').title()\n","            # SHAP for deep models gives importance, not direction, so we present it differently\n","            print(f\" - {feature}: (Relative Importance)\")\n","\n","    except FileNotFoundError as e:\n","        print(f\"Error: A required file was not found. Please run the training script first. Missing file: {e.filename}\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","\n","if __name__ == '__main__':\n","    # Example 1: A validation check for a date in 2024\n","    predict_with_attention_model('2024-05-15')\n","\n","    # Example 2: User can input their own date\n","    print(\"\\n-------------------------------------------------\")\n","    user_date_input = input(\"Enter a date (YYYY-MM-DD) to predict: \")\n","    if user_date_input:\n","        predict_with_attention_model(user_date_input)"]},{"cell_type":"markdown","id":"AvQmzS1HNRhQ","metadata":{"id":"AvQmzS1HNRhQ"},"source":["# New\n"]},{"cell_type":"code","execution_count":null,"id":"mVHArkebjneg","metadata":{"id":"mVHArkebjneg"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# ------------------ Data Sequence Preparation ------------------\n","def prepare_lstm_data(df, feature_cols, target_col, sequence_length=30):\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    X, y = [], []\n","    for i in range(len(df) - sequence_length):\n","        X.append(data[i:i+sequence_length])\n","        y.append(target[i+sequence_length])\n","    return np.array(X), np.array(y, dtype=int)  # Ensure target is int for classification\n","\n","# ------------------ Visualization ------------------\n","def plot_training_history(history):\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Val Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['accuracy'], label='Train Acc')\n","    plt.plot(history.history['val_accuracy'], label='Val Acc')\n","    plt.title('Accuracy Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","# ------------------ Training Function ------------------\n","def train_lstm_classifier(data_filepath):\n","    # Load dataset\n","    df = pd.read_csv(data_filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    target_col = 'Crisis_Target'\n","    exclude_cols = ['Date']\n","    feature_cols = [col for col in df.columns if col not in exclude_cols + [target_col] and df[col].dtype in [np.float64, np.float32, np.int64, np.int32]]\n","\n","    # Normalize only features, NOT target\n","    scaler = MinMaxScaler()\n","    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n","\n","    # Prepare sequences\n","    sequence_length = 30\n","    X, y = prepare_lstm_data(df, feature_cols, target_col, sequence_length)\n","\n","    # NaN/infinite check\n","    print(\"Any NaNs in X?\", np.isnan(X).any(), \"Any NaNs in y?\", np.isnan(y).any())\n","    print(\"Classes in y:\", np.unique(y))\n","\n","    # Split train/test (no shuffle for time series)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","\n","    # Number of classes\n","    num_classes = len(np.unique(y))\n","\n","    # Model Architecture (Classification)\n","    model = Sequential([\n","        Input(shape=(sequence_length, len(feature_cols))),\n","        LSTM(64, return_sequences=True),\n","        Dropout(0.2),\n","        LSTM(32),\n","        Dropout(0.2),\n","        Dense(num_classes, activation='softmax')  # Multi-class output\n","    ])\n","\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    print(model.summary())\n","\n","    # Callbacks\n","    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n","\n","    # Training\n","    history = model.fit(X_train, y_train,\n","                        validation_split=0.2,\n","                        epochs=100,\n","                        batch_size=32,\n","                        callbacks=[early_stop, reduce_lr],\n","                        verbose=2)\n","\n","    # Predictions\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    # Evaluation\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","    cm = confusion_matrix(y_test, y_pred)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()\n","\n","    # Plot training history\n","    plot_training_history(history)\n","\n","    # Save model\n","    model.save(\"water_security_lstm_classifier.keras\")\n","    print(\" Model saved as 'water_security_lstm_classifier.keras'\")\n","\n","    return model, history, scaler\n","\n","# ------------------ Run ------------------\n","if __name__ == \"__main__\":\n","    data_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","    train_lstm_classifier(data_file)\n"]},{"cell_type":"markdown","id":"bqi7HVu5j5OR","metadata":{"id":"bqi7HVu5j5OR"},"source":["## Balanced Classification LSTM"]},{"cell_type":"code","execution_count":null,"id":"_wP73Espsg3-","metadata":{"id":"_wP73Espsg3-"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# -------- Sequence Data Prep --------\n","def prepare_lstm_data(df, feature_cols, target_col, sequence_length=30):\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    X, y = [], []\n","    for i in range(len(df) - sequence_length):\n","        X.append(data[i:i+sequence_length])\n","        y.append(target[i+sequence_length])\n","    return np.array(X), np.array(y, dtype=int)\n","\n","# -------- Plotting --------\n","def plot_training_history(history):\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Val Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['accuracy'], label='Train Acc')\n","    plt.plot(history.history['val_accuracy'], label='Val Acc')\n","    plt.title('Accuracy Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","# -------- Main Training Function --------\n","def train_lstm_classifier(data_filepath):\n","    # Load dataset\n","    df = pd.read_csv(data_filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    target_col = 'Crisis_Target'\n","    exclude_cols = ['Date']\n","    feature_cols = [col for col in df.columns if col not in exclude_cols + [target_col]\n","                    and np.issubdtype(df[col].dtype, np.number)]\n","\n","    # Normalize features\n","    scaler = MinMaxScaler()\n","    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n","\n","    # Prepare sequences\n","    sequence_length = 30\n","    X, y = prepare_lstm_data(df, feature_cols, target_col, sequence_length)\n","\n","    # Check data health\n","    print(\"Any NaNs in X?\", np.isnan(X).any(), \"Any NaNs in y?\", np.isnan(y).any())\n","    print(\"Class distribution in y:\", np.bincount(y))\n","\n","    # Train/test split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","    num_classes = len(np.unique(y_train))\n","\n","    # Compute class weights\n","    class_weights_array = compute_class_weight(class_weight='balanced',\n","                                               classes=np.unique(y_train),\n","                                               y=y_train)\n","    class_weights = dict(enumerate(class_weights_array))\n","    print(\"Computed class weights:\", class_weights)\n","\n","    # Model architecture\n","    model = Sequential([\n","        Input(shape=(sequence_length, len(feature_cols))),\n","        LSTM(64, return_sequences=True),\n","        Dropout(0.2),\n","        LSTM(32),\n","        Dropout(0.2),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    print(model.summary())\n","\n","    # Callbacks\n","    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n","\n","    # Train with class weights\n","    history = model.fit(X_train, y_train,\n","                        validation_split=0.2,\n","                        epochs=100,\n","                        batch_size=32,\n","                        callbacks=[early_stop, reduce_lr],\n","                        class_weight=class_weights,\n","                        verbose=2)\n","\n","    # Evaluate\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","    cm = confusion_matrix(y_test, y_pred)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()\n","\n","    plot_training_history(history)\n","\n","    # Save model\n","    model.save(\"water_security_lstm_classifier_balanced.keras\")\n","    print(\" Model saved as 'water_security_lstm_classifier_balanced.keras'\")\n","\n","    return model, history, scaler\n","\n","# -------- Run --------\n","if __name__ == \"__main__\":\n","    data_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","    train_lstm_classifier(data_file)\n"]},{"cell_type":"markdown","id":"aoQHiL9sjw3u","metadata":{"id":"aoQHiL9sjw3u"},"source":["## Hyperparameter Tuning + Bidirectional/GRU Options"]},{"cell_type":"code","execution_count":null,"id":"XQEwbyPGxPJX","metadata":{"id":"XQEwbyPGxPJX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, Bidirectional\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# For tuning\n","import keras_tuner as kt\n","\n","# ------------------ Data Prep ------------------\n","def prepare_lstm_data(df, feature_cols, target_col, sequence_length=30):\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    X, y = [], []\n","    for i in range(len(df) - sequence_length):\n","        X.append(data[i:i+sequence_length])\n","        y.append(target[i+sequence_length])\n","    return np.array(X), np.array(y, dtype=int)\n","\n","def load_and_prepare_data(filepath, sequence_length):\n","    df = pd.read_csv(filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","    target_col = 'Crisis_Target'\n","    exclude_cols = ['Date']\n","    feature_cols = [col for col in df.columns if col not in exclude_cols+[target_col] and np.issubdtype(df[col].dtype, np.number)]\n","    scaler = MinMaxScaler()\n","    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n","    X, y = prepare_lstm_data(df, feature_cols, target_col, sequence_length)\n","    return X, y, feature_cols, scaler\n","\n","# ------------------ Model Building for Tuner ------------------\n","def build_model(hp, num_classes, seq_len, n_features, arch='LSTM'):\n","    model = Sequential()\n","    model.add(Input(shape=(seq_len, n_features)))\n","    dropout_rate = hp.Float('dropout', 0.2, 0.5, step=0.1, default=0.3)\n","    units1 = hp.Int('units1', 32, 128, step=32, default=64)\n","    units2 = hp.Int('units2', 16, 64, step=16, default=32)\n","    use_bidirectional = hp.Boolean('bidirectional', default=False)\n","    use_gru         = hp.Boolean('use_gru', default=False)\n","\n","    # Choose cell type and direction\n","    RNNLayer = GRU if use_gru else LSTM\n","    def rnn_cell(units, return_sequences=False):\n","        cell = RNNLayer(units, return_sequences=return_sequences)\n","        return Bidirectional(cell) if use_bidirectional else cell\n","\n","    model.add( rnn_cell(units1, return_sequences=True) )\n","    model.add( Dropout(dropout_rate) )\n","    model.add( rnn_cell(units2) )\n","    model.add( Dropout(dropout_rate) )\n","    model.add( Dense(num_classes, activation='softmax') )\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(\n","            learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n","        ),\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    return model\n","\n","# ------------------- Plotting -------------------\n","def plot_training_history(history):\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Val Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['accuracy'], label='Train Acc')\n","    plt.plot(history.history['val_accuracy'], label='Val Acc')\n","    plt.title('Accuracy Over Epochs')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","# ------------------ Main Tuning & Final Training ------------------\n","def run_tuner_and_train(\n","        data_filepath,\n","        max_trials=8,\n","        epochs=40,\n","        batch_size=32,\n","        sequence_length=30,\n","        project_name='lstm_crisis_tune'\n","    ):\n","    X, y, feature_cols, scaler = load_and_prepare_data(data_filepath, sequence_length)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","    num_classes = len(np.unique(y_train))\n","    class_weights = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n","\n","    def model_builder(hp):\n","        return build_model(hp, num_classes, sequence_length, len(feature_cols))\n","\n","    tuner = kt.Hyperband(\n","        model_builder,\n","        objective='val_accuracy',\n","        max_epochs=epochs,\n","        factor=2,\n","        overwrite=True,\n","        directory='.',\n","        project_name=project_name\n","    )\n","\n","    early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n","\n","    tuner.search(\n","        X_train, y_train,\n","        validation_split=0.2,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        class_weight=class_weights,\n","        callbacks=[early_stop, reduce_lr],\n","        verbose=2\n","    )\n","\n","    best_model = tuner.get_best_models(num_models=1)[0]\n","\n","    # Final evaluation\n","    y_pred_probs = best_model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","    cm = confusion_matrix(y_test, y_pred)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()\n","\n","    # Final train for plotting\n","    history = best_model.fit(\n","        X_train, y_train, validation_split=0.2,\n","        epochs=epochs, batch_size=batch_size,\n","        class_weight=class_weights,\n","        callbacks=[early_stop, reduce_lr], verbose=0\n","    )\n","    plot_training_history(history)\n","\n","    best_model.save(\"water_security_lstm_best_tuned.keras\")\n","    print(\" Best model saved as 'water_security_lstm_best_tuned.keras'\")\n","    return best_model, tuner\n","\n","# ------------------- RUN -------------------\n","if __name__ == \"__main__\":\n","    data_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","    run_tuner_and_train(\n","        data_filepath=data_file,\n","        max_trials=10,                  # Number of tunings  increase for more search\n","        epochs=40,                      # Fewer epochs is faster, more may help\n","        batch_size=32,\n","        sequence_length=30,             # Try other values for experimentation too!\n","        project_name='lstm_crisis_tune'\n","    )\n"]},{"cell_type":"markdown","id":"WFmUzJZ7jqCK","metadata":{"id":"WFmUzJZ7jqCK"},"source":["## LSTM with Attention Model"]},{"cell_type":"code","execution_count":null,"id":"6Uae-XEXgghQ","metadata":{"id":"6Uae-XEXgghQ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, MultiHeadAttention,\n","                                   LayerNormalization, GlobalAveragePooling1D, Add)\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# ------------------- Data Preparation -------------------\n","def prepare_lstm_data(df, feature_cols, target_col, sequence_length=30):\n","    \"\"\"Prepare sequences for LSTM input\"\"\"\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    X, y = [], []\n","    for i in range(len(df) - sequence_length):\n","        X.append(data[i:i+sequence_length])\n","        y.append(target[i+sequence_length])\n","    return np.array(X), np.array(y, dtype=int)\n","\n","def load_and_prepare_data(filepath, sequence_length=30):\n","    \"\"\"Load and preprocess the dataset\"\"\"\n","    df = pd.read_csv(filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    target_col = 'Crisis_Target'\n","    exclude_cols = ['Date']\n","    feature_cols = [col for col in df.columns if col not in exclude_cols + [target_col]\n","                    and np.issubdtype(df[col].dtype, np.number)]\n","\n","    # Normalize features\n","    scaler = MinMaxScaler()\n","    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n","\n","    X, y = prepare_lstm_data(df, feature_cols, target_col, sequence_length)\n","    return X, y, feature_cols, scaler\n","\n","# ------------------- Attention Layer -------------------\n","class AttentionLayer(tf.keras.layers.Layer):\n","    \"\"\"Custom Attention Layer for LSTM outputs\"\"\"\n","    def __init__(self, units, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","        self.units = units\n","        self.W = None\n","        self.b = None\n","        self.u = None\n","\n","    def build(self, input_shape):\n","        self.W = self.add_weight(shape=(input_shape[-1], self.units),\n","                               initializer='glorot_uniform', trainable=True)\n","        self.b = self.add_weight(shape=(self.units,),\n","                               initializer='zeros', trainable=True)\n","        self.u = self.add_weight(shape=(self.units,),\n","                               initializer='glorot_uniform', trainable=True)\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        # inputs shape: (batch_size, time_steps, features)\n","        uit = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n","        ait = tf.tensordot(uit, self.u, axes=1)\n","        ait = tf.nn.softmax(ait, axis=-1)\n","        ait = tf.expand_dims(ait, axis=-1)\n","        weighted_input = inputs * ait\n","        output = tf.reduce_sum(weighted_input, axis=1)\n","        return output\n","\n","# ------------------- LSTM with Attention Model -------------------\n","def build_lstm_attention_model(sequence_length, n_features, num_classes,\n","                              lstm_units1=64, lstm_units2=32, attention_units=64,\n","                              dropout_rate=0.3):\n","    \"\"\"Build LSTM with Attention mechanism for crisis classification\"\"\"\n","\n","    inputs = Input(shape=(sequence_length, n_features), name='input_layer')\n","\n","    # First LSTM layer with return_sequences=True\n","    lstm_out1 = LSTM(lstm_units1, return_sequences=True, name='lstm_layer_1')(inputs)\n","    lstm_out1 = Dropout(dropout_rate, name='dropout_1')(lstm_out1)\n","\n","    # Second LSTM layer with return_sequences=True for attention\n","    lstm_out2 = LSTM(lstm_units2, return_sequences=True, name='lstm_layer_2')(lstm_out1)\n","    lstm_out2 = Dropout(dropout_rate, name='dropout_2')(lstm_out2)\n","\n","    # Multi-Head Attention mechanism\n","    attention_out = MultiHeadAttention(\n","        num_heads=4,\n","        key_dim=lstm_units2,\n","        name='multihead_attention'\n","    )(lstm_out2, lstm_out2)\n","\n","    # Layer normalization and residual connection\n","    attention_out = LayerNormalization(name='layer_norm')(attention_out + lstm_out2)\n","\n","    # Custom attention layer for weighted pooling\n","    attention_pooled = AttentionLayer(attention_units, name='custom_attention')(attention_out)\n","\n","    # Dense layers for classification\n","    dense_out = Dense(32, activation='relu', name='dense_hidden')(attention_pooled)\n","    dense_out = Dropout(dropout_rate, name='dropout_final')(dense_out)\n","\n","    # Output layer\n","    outputs = Dense(num_classes, activation='softmax', name='output_layer')(dense_out)\n","\n","    model = Model(inputs=inputs, outputs=outputs, name='LSTM_Attention_Model')\n","    return model\n","\n","# ------------------- Alternative: Transformer-style Attention -------------------\n","def build_transformer_lstm_model(sequence_length, n_features, num_classes,\n","                                lstm_units=64, attention_units=64, dropout_rate=0.3):\n","    \"\"\"Build LSTM with Transformer-style attention\"\"\"\n","\n","    inputs = Input(shape=(sequence_length, n_features))\n","\n","    # LSTM layer\n","    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n","    lstm_out = Dropout(dropout_rate)(lstm_out)\n","\n","    # Multi-head self-attention\n","    attention_out = MultiHeadAttention(num_heads=8, key_dim=attention_units)(lstm_out, lstm_out)\n","    attention_out = Dropout(dropout_rate)(attention_out)\n","\n","    # Residual connection and layer normalization\n","    attention_out = Add()([lstm_out, attention_out])\n","    attention_out = LayerNormalization()(attention_out)\n","\n","    # Feed-forward network\n","    ff_out = Dense(128, activation='relu')(attention_out)\n","    ff_out = Dropout(dropout_rate)(ff_out)\n","    ff_out = Dense(lstm_units)(ff_out)\n","\n","    # Another residual connection\n","    ff_out = Add()([attention_out, ff_out])\n","    ff_out = LayerNormalization()(ff_out)\n","\n","    # Global average pooling and final layers\n","    pooled = GlobalAveragePooling1D()(ff_out)\n","    dense_out = Dense(32, activation='relu')(pooled)\n","    dense_out = Dropout(dropout_rate)(dense_out)\n","    outputs = Dense(num_classes, activation='softmax')(dense_out)\n","\n","    model = Model(inputs=inputs, outputs=outputs, name='Transformer_LSTM_Model')\n","    return model\n","\n","# ------------------- Visualization Functions -------------------\n","def plot_training_history(history):\n","    \"\"\"Plot training and validation metrics\"\"\"\n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","    # Loss\n","    axes[0, 0].plot(history.history['loss'], label='Train Loss')\n","    axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n","    axes[0, 0].set_title('Model Loss')\n","    axes[0, 0].set_xlabel('Epoch')\n","    axes[0, 0].set_ylabel('Loss')\n","    axes[0, 0].legend()\n","\n","    # Accuracy\n","    axes[0, 1].plot(history.history['accuracy'], label='Train Acc')\n","    axes[0, 1].plot(history.history['val_accuracy'], label='Val Acc')\n","    axes[0, 1].set_title('Model Accuracy')\n","    axes[0, 1].set_xlabel('Epoch')\n","    axes[0, 1].set_ylabel('Accuracy')\n","    axes[0, 1].legend()\n","\n","    # Learning Rate\n","    if 'lr' in history.history:\n","        axes[1, 0].plot(history.history['lr'])\n","        axes[1, 0].set_title('Learning Rate')\n","        axes[1, 0].set_xlabel('Epoch')\n","        axes[1, 0].set_ylabel('Learning Rate')\n","\n","    # Additional metrics\n","    if 'f1_score' in history.history:\n","        axes[1, 1].plot(history.history['f1_score'], label='Train F1')\n","        axes[1, 1].plot(history.history['val_f1_score'], label='Val F1')\n","        axes[1, 1].set_title('F1 Score')\n","        axes[1, 1].set_xlabel('Epoch')\n","        axes[1, 1].set_ylabel('F1 Score')\n","        axes[1, 1].legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_attention_weights_analysis(model, X_sample, feature_names, top_k=10):\n","    \"\"\"Visualize attention weights (if custom attention layer is accessible)\"\"\"\n","    # This would require modification to extract attention weights from the model\n","    # For now, we'll create a placeholder visualization\n","    plt.figure(figsize=(12, 8))\n","\n","    # Create dummy attention weights for demonstration\n","    attention_weights = np.random.random((len(feature_names), 30))  # features x time_steps\n","\n","    sns.heatmap(attention_weights[:top_k],\n","                xticklabels=[f'T-{i}' for i in range(30)],\n","                yticklabels=feature_names[:top_k],\n","                cmap='Blues', annot=False)\n","    plt.title('Attention Weights Heatmap (Top Features)')\n","    plt.xlabel('Time Steps')\n","    plt.ylabel('Features')\n","    plt.show()\n","\n","def plot_confusion_matrix_detailed(y_true, y_pred, class_names=None):\n","    \"\"\"Plot detailed confusion matrix\"\"\"\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    if class_names is None:\n","        class_names = [f'Class {i}' for i in range(len(np.unique(y_true)))]\n","\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_names, yticklabels=class_names)\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()\n","\n","# ------------------- Main Training Function -------------------\n","def train_lstm_attention_model(data_filepath, model_type='attention',\n","                              sequence_length=30, epochs=50, batch_size=32):\n","    \"\"\"Complete training pipeline for LSTM with Attention\"\"\"\n","\n","    print(\"Loading and preparing data...\")\n","    X, y, feature_cols, scaler = load_and_prepare_data(data_filepath, sequence_length)\n","\n","    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n","    print(f\"Classes: {np.unique(y)}\")\n","    print(f\"Class distribution: {np.bincount(y)}\")\n","\n","    # Check for NaNs\n","    if np.isnan(X).any() or np.isnan(y).any():\n","        raise ValueError(\"NaN values found in data!\")\n","\n","    # Train-test split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, shuffle=False, random_state=42\n","    )\n","\n","    # Compute class weights for imbalanced data\n","    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","    class_weight_dict = dict(enumerate(class_weights))\n","    print(f\"Class weights: {class_weight_dict}\")\n","\n","    # Build model\n","    num_classes = len(np.unique(y_train))\n","\n","    if model_type == 'attention':\n","        model = build_lstm_attention_model(sequence_length, len(feature_cols), num_classes)\n","    else:\n","        model = build_transformer_lstm_model(sequence_length, len(feature_cols), num_classes)\n","\n","    # Compile model\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","\n","    print(\"\\nModel Architecture:\")\n","    model.summary()\n","\n","    # Callbacks\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n","    )\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6, verbose=1\n","    )\n","\n","    # Train model\n","    print(f\"\\nTraining {model_type} model...\")\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_split=0.2,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        class_weight=class_weight_dict,\n","        callbacks=[early_stopping, reduce_lr],\n","        verbose=1\n","    )\n","\n","    # Evaluate on test set\n","    print(\"\\nEvaluating on test set...\")\n","    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","\n","    # Predictions\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    # Metrics\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    print(f\"\\nTest Results:\")\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","    print(f\"Test F1 Score: {f1:.4f}\")\n","\n","    print(f\"\\nDetailed Classification Report:\")\n","    print(classification_report(y_test, y_pred,\n","                              target_names=['Normal', 'Mild Crisis', 'Moderate Crisis', 'Severe Crisis']))\n","\n","    # Visualizations\n","    plot_training_history(history)\n","    plot_confusion_matrix_detailed(y_test, y_pred,\n","                                 ['Normal', 'Mild Crisis', 'Moderate Crisis', 'Severe Crisis'])\n","\n","    # Save model\n","    model_name = f\"water_crisis_{model_type}_lstm.keras\"\n","    model.save(model_name)\n","    print(f\"\\nModel saved as '{model_name}'\")\n","\n","    return model, history, scaler, {\n","        'test_accuracy': test_accuracy,\n","        'test_f1': f1,\n","        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n","    }\n","\n","# ------------------- Run Training -------------------\n","if __name__ == \"__main__\":\n","    # Path to your LSTM features dataset\n","    data_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","\n","    print(\"=\" * 60)\n","    print(\"TRAINING LSTM WITH ATTENTION MODEL\")\n","    print(\"=\" * 60)\n","\n","    # Train the attention model\n","    model, history, scaler, results = train_lstm_attention_model(\n","        data_filepath=data_file,\n","        model_type='attention',  # or 'transformer'\n","        sequence_length=30,\n","        epochs=50,\n","        batch_size=32\n","    )\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"TRAINING COMPLETED!\")\n","    print(f\"Final Test Accuracy: {results['test_accuracy']:.4f}\")\n","    print(f\"Final Test F1 Score: {results['test_f1']:.4f}\")\n","    print(\"=\" * 60)\n"]},{"cell_type":"markdown","id":"nIpx8FLejjKZ","metadata":{"id":"nIpx8FLejjKZ"},"source":["## Transformer Time Series Classifier (TensorFlow/Keras)"]},{"cell_type":"code","execution_count":null,"id":"hPwUtKKti6gK","metadata":{"id":"hPwUtKKti6gK"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization,\n","                                     MultiHeadAttention, GlobalAveragePooling1D, Add)\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# ------------------- Data Prep -------------------\n","def prepare_lstm_data(df, feature_cols, target_col, sequence_length=30):\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    X, y = [], []\n","    for i in range(len(df) - sequence_length):\n","        X.append(data[i:i+sequence_length])\n","        y.append(target[i+sequence_length])\n","    return np.array(X), np.array(y, dtype=int)\n","\n","def load_and_prepare_data(filepath, sequence_length):\n","    df = pd.read_csv(filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","    target_col = 'Crisis_Target'\n","    exclude_cols = ['Date']\n","    feature_cols = [col for col in df.columns if col not in exclude_cols+[target_col] and np.issubdtype(df[col].dtype, np.number)]\n","    scaler = MinMaxScaler()\n","    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n","    X, y = prepare_lstm_data(df, feature_cols, target_col, sequence_length)\n","    return X, y, feature_cols, scaler\n","\n","# ------------------- Transformer Model -------------------\n","def build_transformer_classification(sequence_length, n_features, num_classes,\n","                                     dense_units=64, dropout_rate=0.3, attn_heads=4, attn_dim=32, n_blocks=2):\n","    inputs = Input(shape=(sequence_length, n_features))\n","    x = Dense(dense_units, activation='relu')(inputs)\n","    for _ in range(n_blocks):\n","        attn_out = MultiHeadAttention(num_heads=attn_heads, key_dim=attn_dim)(x, x)\n","        x = Add()([x, attn_out])\n","        x = LayerNormalization()(x)\n","    x = GlobalAveragePooling1D()(x)\n","    x = Dropout(dropout_rate)(x)\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","    model = Model(inputs, outputs, name=\"Transformer_Crisis_Classifier\")\n","    return model\n","\n","# ------------------- Visualization -------------------\n","def plot_training_history(history):\n","    plt.figure(figsize=(12,5))\n","    plt.subplot(1,2,1)\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Val Loss')\n","    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss Over Epochs'); plt.legend()\n","    plt.subplot(1,2,2)\n","    plt.plot(history.history['accuracy'], label='Train Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n","    plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy Over Epochs'); plt.legend()\n","    plt.tight_layout(); plt.show()\n","\n","def plot_confusion_matrix_detailed(y_true, y_pred, class_names=None):\n","    cm = confusion_matrix(y_true, y_pred)\n","    if class_names is None:\n","        class_names = [str(i) for i in range(len(np.unique(y_true)))]\n","    plt.figure(figsize=(10,8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted'); plt.ylabel('True')\n","    plt.show()\n","\n","# ------------------- Main Training Function -------------------\n","def train_transformer_classifier(data_filepath,\n","                                 sequence_length=30, epochs=50, batch_size=32,\n","                                 dense_units=64, dropout_rate=0.3,\n","                                 attn_heads=4, attn_dim=32, n_blocks=2):\n","    print(\"Loading & preparing data...\")\n","    X, y, feature_cols, scaler = load_and_prepare_data(data_filepath, sequence_length)\n","    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n","    print(f\"Classes: {np.unique(y)}\")\n","    print(f\"Class distribution: {np.bincount(y)}\")\n","\n","    if np.isnan(X).any() or np.isnan(y).any():\n","        raise ValueError(\"Found NaNs in inputs/targets!\")\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n","    num_classes = len(np.unique(y_train))\n","    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","    class_weight_dict = dict(enumerate(class_weights))\n","\n","    model = build_transformer_classification(sequence_length, len(feature_cols), num_classes,\n","                                            dense_units, dropout_rate, attn_heads, attn_dim, n_blocks)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    print(\"\\nModel Architecture:\")\n","    model.summary()\n","\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n","    )\n","    reduce_lr = ReduceLROnPlateau(\n","        monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6, verbose=1\n","    )\n","\n","    print(f\"\\nTraining Transformer model ...\")\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_split=0.2,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        class_weight=class_weight_dict,\n","        callbacks=[early_stopping, reduce_lr],\n","        verbose=1\n","    )\n","\n","    print(\"\\nEvaluating on test set...\")\n","    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    print(f\"\\nTest Results:\")\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","    print(f\"Test F1 Score: {f1:.4f}\")\n","\n","    class_names = ['Normal', 'Mild Crisis', 'Moderate Crisis', 'Severe Crisis']\n","    print(f\"\\nDetailed Classification Report:\")\n","    print(classification_report(y_test, y_pred, target_names=class_names))\n","\n","    plot_training_history(history)\n","    plot_confusion_matrix_detailed(y_test, y_pred, class_names)\n","\n","    model.save(f\"water_crisis_transformer_lstm.keras\")\n","    print(f\"\\nModel saved as 'water_crisis_transformer_lstm.keras'\")\n","\n","    return model, history, scaler, {\n","        'test_accuracy': test_accuracy,\n","        'test_f1': f1,\n","        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n","    }\n","\n","# ------------------- RUN -------------------\n","if __name__ == \"__main__\":\n","    data_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","    print(\"=\"*60)\n","    print(\"TRAINING TRANSFORMER TIME SERIES CLASSIFIER\")\n","    print(\"=\"*60)\n","    model, history, scaler, results = train_transformer_classifier(\n","        data_filepath=data_file,\n","        sequence_length=30,\n","        epochs=50,\n","        batch_size=32,\n","        dense_units=64,\n","        dropout_rate=0.3,\n","        attn_heads=4,\n","        attn_dim=32,\n","        n_blocks=2\n","    )\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"TRAINING COMPLETED!\")\n","    print(f\"Final Test Accuracy: {results['test_accuracy']:.4f}\")\n","    print(f\"Final Test F1 Score: {results['test_f1']:.4f}\")\n","    print(\"=\"*60)\n"]},{"cell_type":"markdown","id":"qRsHP_hHkg4i","metadata":{"id":"qRsHP_hHkg4i"},"source":["## Temporal Convolutional Network (TCN) model"]},{"cell_type":"code","execution_count":null,"id":"6KAiNww4kc3x","metadata":{"id":"6KAiNww4kc3x"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.layers import (Input, Conv1D, Dropout, GlobalAveragePooling1D,\n","                                     Dense, Add, LayerNormalization)\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","# import tensorflow_addons as tfa  # For Focal Loss\n","\n","# ------------------- Data Preparation -------------------\n","def prepare_lstm_data(df, feature_cols, target_col, sequence_length=30):\n","    X, y = [], []\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    for i in range(len(df) - sequence_length):\n","        X.append(data[i:i+sequence_length])\n","        y.append(target[i+sequence_length])\n","    return np.array(X), np.array(y, dtype=int)\n","\n","def load_and_prepare_data(filepath, sequence_length):\n","    df = pd.read_csv(filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    target_col = 'Crisis_Target'\n","    feature_cols = [col for col in df.columns\n","                    if col not in ['Date', target_col] and np.issubdtype(df[col].dtype, np.number)]\n","\n","    scaler = MinMaxScaler()\n","    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n","\n","    X, y = prepare_lstm_data(df, feature_cols, target_col, sequence_length)\n","    return X, y, feature_cols, scaler\n","\n","# ------------------- Residual TCN Block -------------------\n","def tcn_block(x_in, filters, kernel_size, dilation_rate, dropout_rate):\n","    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x_in)\n","    x = Dropout(dropout_rate)(x)\n","    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n","    x = Dropout(dropout_rate)(x)\n","    if x_in.shape[-1] != filters:\n","        x_in = Conv1D(filters, 1, padding='same')(x_in)\n","    out = Add()([x_in, x])  # Residual connection\n","    out = LayerNormalization()(out)\n","    return out\n","\n","# ------------------- Build Model -------------------\n","def build_tcn_model(sequence_length, n_features, num_classes, dropout_rate=0.3):\n","    inputs = Input(shape=(sequence_length, n_features))\n","    x = tcn_block(inputs, 64, 3, dilation_rate=1, dropout_rate=dropout_rate)\n","    x = tcn_block(x, 64, 3, dilation_rate=2, dropout_rate=dropout_rate)\n","    x = tcn_block(x, 64, 3, dilation_rate=4, dropout_rate=dropout_rate)\n","    x = GlobalAveragePooling1D()(x)\n","    x = Dense(32, activation='relu')(x)\n","    x = Dropout(dropout_rate)(x)\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","    return Model(inputs, outputs, name=\"Enhanced_TCN_Crisis_Classifier\")\n","\n","# ------------------- Visualization -------------------\n","def plot_training_history(history):\n","    plt.figure(figsize=(12,5))\n","    plt.subplot(1,2,1)\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Val Loss')\n","    plt.title('Loss'); plt.xlabel('Epochs'); plt.legend()\n","    plt.subplot(1,2,2)\n","    plt.plot(history.history['accuracy'], label='Train Acc')\n","    plt.plot(history.history['val_accuracy'], label='Val Acc')\n","    plt.title('Accuracy'); plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend()\n","    plt.tight_layout(); plt.show()\n","\n","def plot_confusion_matrix_detailed(y_true, y_pred, class_names=None):\n","    cm = confusion_matrix(y_true, y_pred)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_names, yticklabels=class_names)\n","    plt.xlabel('Predicted'); plt.ylabel('True')\n","    plt.show()\n","\n","# ------------------- Training -------------------\n","def train_tcn_classifier(data_filepath, sequence_length=30, epochs=50, batch_size=32, dropout_rate=0.3):\n","    print(\"Loading & preparing data...\")\n","    X, y, feature_cols, scaler = load_and_prepare_data(data_filepath, sequence_length)\n","    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n","    print(f\"Classes: {np.unique(y)} | Distribution: {np.bincount(y)}\")\n","\n","    #  Chronological split (no shuffle before split)\n","    split_index = int(len(X) * 0.8)\n","    X_train, X_test = X[:split_index], X[split_index:]\n","    y_train, y_test = y[:split_index], y[split_index:]\n","\n","    num_classes = len(np.unique(y_train))\n","    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","    class_weight_dict = dict(enumerate(class_weights))\n","    print(f\"Class weights: {class_weight_dict}\")\n","\n","    model = build_tcn_model(sequence_length, len(feature_cols), num_classes, dropout_rate)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","                  loss='sparse_categorical_crossentropy',  # Replaced Focal Loss\n","                  metrics=['accuracy'])\n","\n","    print(\"\\nModel Summary:\")\n","    model.summary()\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-6)\n","\n","    print(\"\\nTraining Enhanced TCN...\")\n","    history = model.fit(X_train, y_train,\n","                        validation_split=0.2,\n","                        epochs=epochs,\n","                        batch_size=batch_size,\n","                        class_weight=class_weight_dict,\n","                        callbacks=[early_stopping, reduce_lr],\n","                        verbose=1)\n","\n","    print(\"\\nEvaluating on test set...\")\n","    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f} | Test F1: {f1:.4f}\")\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred, target_names=['Normal', 'Mild Crisis', 'Moderate Crisis', 'Severe Crisis']))\n","\n","    plot_training_history(history)\n","    plot_confusion_matrix_detailed(y_test, y_pred, ['Normal', 'Mild Crisis', 'Moderate Crisis', 'Severe Crisis'])\n","\n","    model.save(\"water_crisis_tcn_enhanced.keras\")\n","    print(\"\\nModel saved as 'water_crisis_tcn_enhanced.keras'\")\n","\n","    return model, history, scaler\n","\n","# ------------------- Run -------------------\n","if __name__ == \"__main__\":\n","    data_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","    model, history, scaler = train_tcn_classifier(\n","        data_filepath=data_file,\n","        sequence_length=30,  # try increasing to 60/90 for more context\n","        epochs=50,\n","        batch_size=32,\n","        dropout_rate=0.3\n","    )"]},{"cell_type":"markdown","id":"XUQVPvJkp2Dx","metadata":{"id":"XUQVPvJkp2Dx"},"source":["Feature engineering hooks so you can extend your dataset.\n","\n","Longer sequence length support (e.g., 90 days).\n","\n","Stronger class imbalance handling  focal loss (custom TF 2.19-compatible) + optional oversampling.\n","\n","Hybrid architecture: TCN+LSTM+Attention for better temporal feature capture.\n","\n","Chronological split to respect time ordering."]},{"cell_type":"code","execution_count":null,"id":"epa6qPAlpswi","metadata":{"id":"epa6qPAlpswi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.layers import (Input, Conv1D, Dropout, GlobalAveragePooling1D,\n","                                     Dense, Add, LayerNormalization, LSTM, MultiHeadAttention)\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# ------------------- Custom Focal Loss (TF 2.19 Safe) -------------------\n","def focal_loss(alpha=0.25, gamma=2.0):\n","    def loss_fn(y_true, y_pred):\n","        y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])\n","        y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)\n","        cross_entropy = -y_true * tf.math.log(y_pred)\n","        weight = alpha * tf.math.pow(1 - y_pred, gamma)\n","        return tf.reduce_mean(tf.reduce_sum(weight * cross_entropy, axis=-1))\n","    return loss_fn\n","\n","# ------------------- Data Preparation -------------------\n","def prepare_sequences(df, feature_cols, target_col, seq_len=90):\n","    X, y = [], []\n","    data = df[feature_cols].values\n","    target = df[target_col].values\n","    for i in range(len(df) - seq_len):\n","        X.append(data[i:i+seq_len])\n","        y.append(target[i+seq_len])\n","    return np.array(X), np.array(y, dtype=int)\n","\n","def load_and_prepare(filepath, seq_len=90):\n","    df = pd.read_csv(filepath)\n","    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n","    df = df.sort_values('Date').reset_index(drop=True)\n","\n","    target_col = 'Crisis_Target'\n","    num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != target_col]\n","\n","    scaler = MinMaxScaler()\n","    df[num_cols] = scaler.fit_transform(df[num_cols])\n","\n","    X, y = prepare_sequences(df, num_cols, target_col, seq_len)\n","    return X, y, num_cols, scaler\n","\n","# ------------------- Hybrid Model: TCN + LSTM + Attention -------------------\n","def residual_tcn(x_in, filters, ksize, dilation, dr):\n","    x = Conv1D(filters, ksize, padding='causal', dilation_rate=dilation, activation='relu')(x_in)\n","    x = Dropout(dr)(x)\n","    x = Conv1D(filters, ksize, padding='causal', dilation_rate=dilation, activation='relu')(x)\n","    x = Dropout(dr)(x)\n","    if x_in.shape[-1] != filters:\n","        x_in = Conv1D(filters, 1, padding='same')(x_in)\n","    out = Add()([x_in, x])\n","    return LayerNormalization()(out)\n","\n","def build_hybrid(seq_len, n_feats, n_classes, dr=0.3):\n","    inputs = Input(shape=(seq_len, n_feats))\n","    x = residual_tcn(inputs, 64, 3, 1, dr)\n","    x = residual_tcn(x, 64, 3, 2, dr)\n","    x = residual_tcn(x, 64, 3, 4, dr)\n","    x = LSTM(64, return_sequences=True)(x)\n","    attn = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n","    x = Add()([x, attn])\n","    x = LayerNormalization()(x)\n","    x = GlobalAveragePooling1D()(x)\n","    x = Dense(64, activation='relu')(x)\n","    x = Dropout(dr)(x)\n","    outputs = Dense(n_classes, activation='softmax')(x)\n","    return Model(inputs, outputs)\n","\n","# ------------------- Training Plot -------------------\n","def plot_history(hist):\n","    plt.figure(figsize=(12,5))\n","    plt.subplot(1,2,1)\n","    plt.plot(hist.history['loss'], label='Train Loss')\n","    plt.plot(hist.history['val_loss'], label='Val Loss')\n","    plt.legend(); plt.title('Loss')\n","    plt.subplot(1,2,2)\n","    plt.plot(hist.history['accuracy'], label='Train Acc')\n","    plt.plot(hist.history['val_accuracy'], label='Val Acc')\n","    plt.legend(); plt.title('Accuracy')\n","    plt.show()\n","\n","def plot_confusion(y_true, y_pred, labels):\n","    cm = confusion_matrix(y_true, y_pred)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n","    plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()\n","\n","# ------------------- Main Training -------------------\n","def train_pipeline(csv_path, seq_len=90, epochs=50, batch_size=32):\n","    X, y, feats, scaler = load_and_prepare(csv_path, seq_len)\n","    split_idx = int(len(X) * 0.8)   # chronological split\n","    X_train, X_test = X[:split_idx], X[split_idx:]\n","    y_train, y_test = y[:split_idx], y[split_idx:]\n","\n","    n_classes = len(np.unique(y_train))\n","    cw = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","    cw_dict = dict(enumerate(cw))\n","\n","    model = build_hybrid(seq_len, len(feats), n_classes)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n","                  loss=focal_loss(alpha=0.25, gamma=2.0),\n","                  metrics=['accuracy'])\n","    model.summary()\n","\n","    es = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n","    lr_red = ReduceLROnPlateau(monitor='val_loss', patience=6, factor=0.5, min_lr=1e-6)\n","\n","    hist = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs,\n","                     batch_size=batch_size, class_weight=cw_dict,\n","                     callbacks=[es, lr_red], verbose=2)\n","\n","    print(\"\\nEvaluating on Test Data...\")\n","    y_prob = model.predict(X_test)\n","    y_pred = np.argmax(y_prob, axis=1)\n","    test_f1 = f1_score(y_test, y_pred, average='weighted')\n","    print(classification_report(y_test, y_pred, target_names=['Normal', 'Mild', 'Moderate', 'Severe']))\n","    print(f\"Weighted F1: {test_f1:.4f}\")\n","\n","    plot_history(hist)\n","    plot_confusion(y_test, y_pred, ['Normal', 'Mild', 'Moderate', 'Severe'])\n","\n","    model.save(\"water_crisis_hybrid.keras\")\n","    return model, scaler\n","\n","# ------------------- Run -------------------\n","if __name__ == \"__main__\":\n","    csv_file = \"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_lstm_features_dataset.csv\"\n","    train_pipeline(csv_file, seq_len=90, epochs=50, batch_size=32)\n"]},{"cell_type":"markdown","id":"r0NT9YyDLtoJ","metadata":{"id":"r0NT9YyDLtoJ"},"source":["#NEW"]},{"cell_type":"code","execution_count":null,"id":"GuLnnpHWLvjC","metadata":{"id":"GuLnnpHWLvjC"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n","import matplotlib.pyplot as plt\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","    print(\"Data head:\\n\", df.head())\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X = df.drop('Crisis_Target_V2', axis=1)\n","y = df['Crisis_Target_V2']\n","\n","# The target variable is categorical (e.g., 1, 2, 3).\n","# Neural networks work best with 0-indexed targets.\n","# We subtract 1 to make it 0, 1, 2...\n","num_classes = len(y.unique())\n","print(f\"\\nNumber of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values before adjustment: {df['Crisis_Target_V2'].unique()}\")\n","print(f\"Unique target values after adjustment: {y.unique()}\")\n","\n","\n","# Scale the features to be between 0 and 1. This is crucial for neural network performance.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X)\n","\n","# --- 3. Create Time-Series Sequences ---\n","\n","# This function converts our time series data into sequences of a specified\n","# length (time_step). The model will use a sequence of 'time_step' days\n","# to predict the crisis level on the following day.\n","def create_dataset(X, y, time_step=1):\n","    \"\"\"\n","    Creates sequences of data for time-series forecasting.\n","\n","    Args:\n","        X (np.array): Array of features.\n","        y (np.array): Array of target values.\n","        time_step (int): The number of previous time steps to use as input variables\n","                         to predict the next time period.\n","\n","    Returns:\n","        tuple: A tuple containing the reshaped feature array (X) and the target array (y).\n","    \"\"\"\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        # Take a sequence of 'time_step' features\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        # The target is the value at the end of the sequence\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","# We will use the data from the last 60 days to predict the crisis on the 61st day.\n","time_step = 60\n","X_seq, y_seq = create_dataset(X_scaled, y.values, time_step)\n","\n","print(f\"\\nData reshaped into sequences of {time_step} days.\")\n","print(\"X_seq shape:\", X_seq.shape)\n","print(\"y_seq shape:\", y_seq.shape)\n","\n","\n","# --- 4. Split Data into Training and Testing Sets ---\n","\n","# For time-series data, we should not shuffle the data. We split it chronologically.\n","# We'll use 80% of the data for training and 20% for testing.\n","train_size = int(len(X_seq) * 0.8)\n","test_size = len(X_seq) - train_size\n","\n","X_train, X_test = X_seq[0:train_size], X_seq[train_size:len(X_seq)]\n","y_train, y_test = y_seq[0:train_size], y_seq[train_size:len(y_seq)]\n","\n","print(\"\\nData split into training and testing sets:\")\n","print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_test shape:\", y_test.shape)\n","\n","\n","# --- 5. Build the LSTM Model ---\n","\n","# We will build a stacked LSTM model with Dropout layers to prevent overfitting.\n","# This architecture is powerful for learning complex temporal patterns.\n","model = Sequential()\n","\n","# Input layer specifies the shape of the input data\n","model.add(Input(shape=(time_step, X_train.shape[2])))\n","\n","# First LSTM layer with 100 units. `return_sequences=True` is needed to\n","# pass the output of this layer as a sequence to the next layer.\n","model.add(LSTM(100, return_sequences=True))\n","model.add(Dropout(0.2)) # Dropout layer to prevent overfitting\n","\n","# Second LSTM layer\n","model.add(LSTM(100, return_sequences=True))\n","model.add(Dropout(0.2))\n","\n","# Third LSTM layer. `return_sequences=False` as this is the last LSTM layer.\n","model.add(LSTM(50))\n","model.add(Dropout(0.2))\n","\n","# A standard Dense layer for further processing\n","model.add(Dense(50, activation='relu'))\n","\n","# The final output layer. The number of units equals the number of crisis classes.\n","# 'softmax' is used for multi-class classification.\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","# Compile the model\n","# 'adam' is an efficient optimizer.\n","# 'sparse_categorical_crossentropy' is used as the loss function for multi-class\n","# classification problems where the labels are integers.\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Display the model's architecture\n","model.summary()\n","\n","\n","# --- 6. Train the Model ---\n","\n","print(\"\\n--- Training the model ---\")\n","# We train the model on the training data.\n","# `validation_data` is used to evaluate the model's performance on the test set after each epoch.\n","history = model.fit(\n","    X_train,\n","    y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=50, # An epoch is one complete pass through the entire training dataset.\n","    batch_size=64, # The number of samples to work through before updating the internal model parameters.\n","    verbose=1\n",")\n","\n","\n","# --- 7. Evaluate the Model ---\n","\n","print(\"\\n--- Evaluating the model ---\")\n","\n","# Make predictions on the test set\n","y_pred_probs = model.predict(X_test)\n","y_pred = np.argmax(y_pred_probs, axis=1) # Get the class with the highest probability\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\nModel Accuracy on Test Data: {accuracy:.4f}\")\n","\n","# Display a detailed classification report\n","print(\"\\nClassification Report:\")\n","# We add 1 back to the labels to see them in their original format (1, 2, ...)\n","report = classification_report(y_test + 1, y_pred + 1)\n","print(report)\n","\n","\n","# --- 8. Plot Training History ---\n","\n","print(\"\\n--- Plotting training history ---\")\n","plt.style.use('seaborn-v0_8-whitegrid')\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n","\n","# Plot training & validation accuracy values\n","ax1.plot(history.history['accuracy'], label='Train Accuracy', color='dodgerblue', linewidth=2)\n","ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', color='darkorange', linestyle='--', linewidth=2)\n","ax1.set_title('Model Accuracy', fontsize=16, fontweight='bold')\n","ax1.set_ylabel('Accuracy', fontsize=12)\n","ax1.legend(loc='lower right')\n","ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n","\n","# Plot training & validation loss values\n","ax2.plot(history.history['loss'], label='Train Loss', color='dodgerblue', linewidth=2)\n","ax2.plot(history.history['val_loss'], label='Validation Loss', color='darkorange', linestyle='--', linewidth=2)\n","ax2.set_title('Model Loss', fontsize=16, fontweight='bold')\n","ax2.set_ylabel('Loss', fontsize=12)\n","ax2.set_xlabel('Epoch', fontsize=12)\n","ax2.legend(loc='upper right')\n","ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n","\n","plt.tight_layout()\n","plt.show()\n","print(\"Plot displayed. You can now close the plot window.\")\n"]},{"cell_type":"code","execution_count":null,"id":"aG5NdqCjRjvZ","metadata":{"colab":{"background_save":true},"id":"aG5NdqCjRjvZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 60\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train the Model with Cross-Validation ---\n","\n","# Use TimeSeriesSplit for robust cross-validation on time-series data.\n","# This creates multiple train/test splits, ensuring the model is evaluated\n","# across different time periods and preventing data leakage.\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","# Store scores and histories from each fold\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    # This is crucial for forcing the model to pay attention to rare classes.\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the LSTM Model ---\n","    # We rebuild the model in each fold to ensure it starts fresh.\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[2])),\n","        LSTM(100, return_sequences=True),\n","        Dropout(0.3),\n","        LSTM(100, return_sequences=True),\n","        Dropout(0.3),\n","        LSTM(50),\n","        Dropout(0.3),\n","        Dense(50, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # --- Use Early Stopping to Prevent Overfitting ---\n","    # This stops training when the validation loss stops improving.\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=10, # Number of epochs with no improvement to wait before stopping\n","        verbose=1,\n","        restore_best_weights=True # Restores model weights from the epoch with the best val_loss\n","    )\n","\n","    # --- Train the Model ---\n","    history = model.fit(\n","        X_train,\n","        y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100, # Set a high number of epochs; EarlyStopping will find the optimal number\n","        batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","\n","    # --- Evaluate on the test fold ---\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    # Store results for final evaluation\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","# Specify all possible labels to ensure the report includes them all.\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n"]},{"cell_type":"code","execution_count":null,"id":"VjUsmjrqNYe-","metadata":{"id":"VjUsmjrqNYe-"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","# **TUNED HYPERPARAMETER**: Changed time_step to 90\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train the Model with Cross-Validation ---\n","\n","# Use TimeSeriesSplit for robust cross-validation on time-series data.\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","# Store scores and histories from each fold\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the LSTM Model ---\n","    # **TUNED HYPERPARAMETERS**: Increased LSTM units and adjusted Dropout.\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[2])),\n","        LSTM(150, return_sequences=True), # Increased units\n","        Dropout(0.2), # Adjusted dropout\n","        LSTM(150, return_sequences=True), # Increased units\n","        Dropout(0.2), # Adjusted dropout\n","        LSTM(64),\n","        Dropout(0.2),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # --- Use Early Stopping to Prevent Overfitting ---\n","    # **TUNED HYPERPARAMETER**: Increased patience\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=20,\n","        verbose=1,\n","        restore_best_weights=True\n","    )\n","\n","    # --- Train the Model ---\n","    history = model.fit(\n","        X_train,\n","        y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100,\n","        batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","\n","    # --- Evaluate on the test fold ---\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    # Store results for final evaluation\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Model on All Data for Prediction ---\")\n","\n","# --- Handle Class Imbalance for the full dataset ---\n","final_class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_seq),\n","    y=y_seq\n",")\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","print(\"Final Class Weights:\", final_class_weights_dict)\n","\n","# --- Build the final model with tuned hyperparameters ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[2])),\n","    LSTM(150, return_sequences=True), # Increased units\n","    Dropout(0.2), # Adjusted dropout\n","    LSTM(150, return_sequences=True), # Increased units\n","    Dropout(0.2), # Adjusted dropout\n","    LSTM(64),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model on the entire dataset ---\n","final_model.fit(\n","    X_seq,\n","    y_seq,\n","    epochs=30, # A reasonable number of epochs based on previous runs\n","    batch_size=64,\n","    class_weight=final_class_weights_dict,\n","    verbose=1\n",")\n","\n","# --- Prepare the last 90 days of data for prediction ---\n","last_days = X_scaled[-time_step:]\n","prediction_input = last_days.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make the prediction ---\n","prediction_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(prediction_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n","\n"]},{"cell_type":"markdown","id":"ZzrA6NIpoQ4e","metadata":{"id":"ZzrA6NIpoQ4e"},"source":["## Bidirectional LSTM"]},{"cell_type":"code","execution_count":null,"id":"paw4rVO0oMgB","metadata":{"id":"paw4rVO0oMgB"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","# **TUNED HYPERPARAMETER**: Changed time_step to 90\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train the Model with Cross-Validation ---\n","\n","# Use TimeSeriesSplit for robust cross-validation on time-series data.\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","# Store scores and histories from each fold\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Bidirectional LSTM Model ---\n","    # This architecture can capture patterns from both past-to-future and future-to-past.\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[2])),\n","        Bidirectional(LSTM(150, return_sequences=True)), # Using Bidirectional wrapper\n","        Dropout(0.2),\n","        Bidirectional(LSTM(150, return_sequences=True)), # Using Bidirectional wrapper\n","        Dropout(0.2),\n","        Bidirectional(LSTM(64)), # Using Bidirectional wrapper\n","        Dropout(0.2),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # --- Use Early Stopping to Prevent Overfitting ---\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=20,\n","        verbose=1,\n","        restore_best_weights=True\n","    )\n","\n","    # --- Train the Model ---\n","    history = model.fit(\n","        X_train,\n","        y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100,\n","        batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","\n","    # --- Evaluate on the test fold ---\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    # Store results for final evaluation\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Model on All Data for Prediction ---\")\n","\n","# --- Handle Class Imbalance for the full dataset ---\n","final_class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_seq),\n","    y=y_seq\n",")\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","print(\"Final Class Weights:\", final_class_weights_dict)\n","\n","# --- Build the final model with Bidirectional LSTMs ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[2])),\n","    Bidirectional(LSTM(150, return_sequences=True)),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(150, return_sequences=True)),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(64)),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model on the entire dataset ---\n","final_model.fit(\n","    X_seq,\n","    y_seq,\n","    epochs=30, # A reasonable number of epochs based on previous runs\n","    batch_size=64,\n","    class_weight=final_class_weights_dict,\n","    verbose=1\n",")\n","\n","# --- Prepare the last 90 days of data for prediction ---\n","last_days = X_scaled[-time_step:]\n","prediction_input = last_days.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make the prediction ---\n","prediction_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(prediction_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n","\n"]},{"cell_type":"code","source":["import pandas as pd\n","df=pd.read_csv(\"/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv\")\n","df.shape\n"],"metadata":{"id":"XgFlccUuwFZU"},"id":"XgFlccUuwFZU","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"Jf7nN3PP0L56","metadata":{"id":"Jf7nN3PP0L56"},"source":["## Attention Mechanism"]},{"cell_type":"code","execution_count":null,"id":"LDWyq5Fn0OkF","metadata":{"id":"LDWyq5Fn0OkF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional, Layer\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- Custom Attention Layer ---\n","# This layer allows the model to focus on the most important parts of the input sequence.\n","class Attention(Layer):\n","    def __init__(self, **kwargs):\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n","                                 initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n","                                 initializer=\"zeros\")\n","        super(Attention, self).build(input_shape)\n","\n","    def call(self, x):\n","        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n","        at = K.softmax(et)\n","        at = K.expand_dims(at, axis=-1)\n","        output = x * at\n","        return K.sum(output, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n","\n","    def get_config(self):\n","        return super(Attention, self).get_config()\n","\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train the Model with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Bidirectional LSTM Model with Attention ---\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[2])),\n","        Bidirectional(LSTM(150, return_sequences=True)),\n","        Dropout(0.2),\n","        Bidirectional(LSTM(150, return_sequences=True)),\n","        Dropout(0.2),\n","        Attention(), # Applying the custom Attention layer\n","        Dense(64, activation='relu'),\n","        Dropout(0.2),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # --- Use Early Stopping to Prevent Overfitting ---\n","    early_stopping = EarlyStopping(\n","        monitor='val_loss',\n","        patience=20,\n","        verbose=1,\n","        restore_best_weights=True\n","    )\n","\n","    # --- Train the Model ---\n","    history = model.fit(\n","        X_train,\n","        y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100,\n","        batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","\n","    # --- Evaluate on the test fold ---\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    # Store results for final evaluation\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Model on All Data for Prediction ---\")\n","\n","# --- Handle Class Imbalance for the full dataset ---\n","final_class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_seq),\n","    y=y_seq\n",")\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","print(\"Final Class Weights:\", final_class_weights_dict)\n","\n","# --- Build the final model with Bidirectional LSTMs and Attention ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[2])),\n","    Bidirectional(LSTM(150, return_sequences=True)),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(150, return_sequences=True)),\n","    Dropout(0.2),\n","    Attention(),\n","    Dense(64, activation='relu'),\n","    Dropout(0.2),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model on the entire dataset ---\n","final_model.fit(\n","    X_seq,\n","    y_seq,\n","    epochs=30,\n","    batch_size=64,\n","    class_weight=final_class_weights_dict,\n","    verbose=1\n",")\n","\n","# --- Prepare the last 90 days of data for prediction ---\n","last_days = X_scaled[-time_step:]\n","prediction_input = last_days.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make the prediction ---\n","prediction_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(prediction_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"RFE2twedZNPS","metadata":{"id":"RFE2twedZNPS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Bidirectional\n","from tensorflow.keras.callbacks import EarlyStopping\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train Ensemble Model with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Ensemble Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train_seq, X_test_seq = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Model 1: Bidirectional LSTM ---\n","    print(\"\\n--- Training Bidirectional LSTM ---\")\n","    lstm_model = Sequential([\n","        Input(shape=(time_step, X_train_seq.shape[2])),\n","        Bidirectional(LSTM(150, return_sequences=True)),\n","        Dropout(0.2),\n","        Bidirectional(LSTM(150, return_sequences=True)),\n","        Dropout(0.2),\n","        Bidirectional(LSTM(64)),\n","        Dropout(0.2),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    lstm_model.fit(\n","        X_train_seq, y_train,\n","        validation_data=(X_test_seq, y_test),\n","        epochs=100, batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    lstm_pred_probs = lstm_model.predict(X_test_seq)\n","\n","    # --- Model 2: XGBoost ---\n","    print(\"\\n--- Training XGBoost Classifier ---\")\n","    # Reshape data for XGBoost (it expects 2D input)\n","    X_train_xgb = X_train_seq.reshape(X_train_seq.shape[0], -1)\n","    X_test_xgb = X_test_seq.reshape(X_test_seq.shape[0], -1)\n","\n","    xgb_model = xgb.XGBClassifier(\n","        objective='multi:softprob',\n","        num_class=num_classes,\n","        eval_metric='mlogloss',\n","        use_label_encoder=False,\n","        n_estimators=200,\n","        learning_rate=0.1,\n","        max_depth=5\n","    )\n","    xgb_model.fit(X_train_xgb, y_train)\n","    xgb_pred_probs = xgb_model.predict_proba(X_test_xgb)\n","\n","    # --- FIX for XGBoost prediction shape mismatch ---\n","    # If a fold's training data is missing a class, XGBoost will output fewer columns.\n","    # This code block ensures the prediction array has a column for every class.\n","    if xgb_pred_probs.shape[1] < num_classes:\n","        # Get the classes the model was actually trained on\n","        trained_classes = xgb_model.classes_\n","        # Create a new array with zeros, with the correct shape\n","        full_probs = np.zeros((xgb_pred_probs.shape[0], num_classes))\n","        # Place the predicted probabilities into the correct columns\n","        full_probs[:, trained_classes] = xgb_pred_probs\n","        xgb_pred_probs = full_probs\n","\n","\n","    # --- Ensemble Prediction ---\n","    # Average the probabilities from both models\n","    ensemble_pred_probs = (lstm_pred_probs + xgb_pred_probs) / 2.0\n","    ensemble_pred = np.argmax(ensemble_pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, ensemble_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nEnsemble Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(ensemble_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall ENSEMBLE Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Ensemble Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Ensemble Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Ensemble Model on All Data for Prediction ---\")\n","\n","# --- Final LSTM Model ---\n","print(\"\\n--- Training Final Bidirectional LSTM ---\")\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","final_lstm_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[2])),\n","    Bidirectional(LSTM(150, return_sequences=True)), Dropout(0.2),\n","    Bidirectional(LSTM(150, return_sequences=True)), Dropout(0.2),\n","    Bidirectional(LSTM(64)), Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","final_lstm_model.fit(X_seq, y_seq, epochs=30, batch_size=64, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Final XGBoost Model ---\n","print(\"\\n--- Training Final XGBoost Classifier ---\")\n","X_seq_xgb = X_seq.reshape(X_seq.shape[0], -1)\n","final_xgb_model = xgb.XGBClassifier(\n","    objective='multi:softprob', num_class=num_classes, eval_metric='mlogloss',\n","    use_label_encoder=False, n_estimators=200, learning_rate=0.1, max_depth=5\n",")\n","final_xgb_model.fit(X_seq_xgb, y_seq)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input_seq = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","prediction_input_xgb = last_days_scaled.reshape(1, -1)\n","\n","# --- Make final ensemble prediction ---\n","final_lstm_pred_probs = final_lstm_model.predict(prediction_input_seq)\n","final_xgb_pred_probs = final_xgb_model.predict_proba(prediction_input_xgb)\n","final_ensemble_probs = (final_lstm_pred_probs + final_xgb_pred_probs) / 2.0\n","predicted_class = np.argmax(final_ensemble_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      ENSEMBLE PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"]},{"cell_type":"code","execution_count":null,"id":"NfDz6K5WlC7Z","metadata":{"id":"NfDz6K5WlC7Z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Layer\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- Transformer Model Components ---\n","\n","class TransformerBlock(Layer):\n","    \"\"\"\n","    A custom Transformer block layer.\n","    This includes multi-head self-attention, feed-forward network, and layer normalization.\n","    \"\"\"\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n","        super(TransformerBlock, self).__init__(**kwargs)\n","        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = Sequential([\n","            Dense(ff_dim, activation=\"relu\"),\n","            Dense(embed_dim)\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training=False):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.att.key_dim,\n","            \"num_heads\": self.att.num_heads,\n","            \"ff_dim\": self.ffn.layers[0].units,\n","            \"rate\": self.dropout1.rate,\n","        })\n","        return config\n","\n","class TokenAndPositionEmbedding(Layer):\n","    \"\"\"\n","    A custom layer to add positional information to the input sequences.\n","    \"\"\"\n","    def __init__(self, maxlen, embed_dim, **kwargs):\n","        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n","        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.maxlen = maxlen\n","        self.embed_dim = embed_dim\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-2]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"maxlen\": self.maxlen,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train Transformer Model with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Transformer Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Transformer Model ---\n","    embed_dim = X_train.shape[-1] # Embedding size for each token\n","    num_heads = 4  # Number of attention heads\n","    ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n","\n","    inputs = Input(shape=(time_step, embed_dim))\n","    embedding_layer = TokenAndPositionEmbedding(time_step, embed_dim)\n","    x = embedding_layer(inputs)\n","    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","    x = transformer_block(x)\n","    x = GlobalAveragePooling1D()(x)\n","    x = Dropout(0.2)(x)\n","    x = Dense(64, activation=\"relu\")(x)\n","    x = Dropout(0.2)(x)\n","    outputs = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    transformer_model = Model(inputs=inputs, outputs=outputs)\n","    transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    transformer_model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100, batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = transformer_model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nTransformer Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall TRANSFORMER Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Transformer Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Transformer Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Transformer Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","embed_dim = X_seq.shape[-1]\n","num_heads = 4\n","ff_dim = 128\n","\n","final_inputs = Input(shape=(time_step, embed_dim))\n","final_embedding_layer = TokenAndPositionEmbedding(time_step, embed_dim)\n","fx = final_embedding_layer(final_inputs)\n","final_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","fx = final_transformer_block(fx)\n","fx = GlobalAveragePooling1D()(fx)\n","fx = Dropout(0.2)(fx)\n","fx = Dense(64, activation=\"relu\")(fx)\n","fx = Dropout(0.2)(fx)\n","final_outputs = Dense(num_classes, activation=\"softmax\")(fx)\n","\n","final_transformer_model = Model(inputs=final_inputs, outputs=final_outputs)\n","final_transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_transformer_model.fit(X_seq, y_seq, epochs=30, batch_size=64, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_transformer_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      TRANSFORMER PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"]},{"cell_type":"code","execution_count":null,"id":"q9ZvE78c1V9s","metadata":{"id":"q9ZvE78c1V9s"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, LSTM\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train CNN-LSTM Hybrid Model with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with CNN-LSTM Hybrid Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the CNN-LSTM Hybrid Model ---\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[-1])),\n","        # CNN layers for feature extraction from sequences\n","        Conv1D(filters=64, kernel_size=3, activation='relu'),\n","        Conv1D(filters=64, kernel_size=3, activation='relu'),\n","        MaxPooling1D(pool_size=2),\n","        Dropout(0.2),\n","        # Bidirectional LSTM layers to learn from the extracted features\n","        Bidirectional(LSTM(100, return_sequences=True)),\n","        Dropout(0.2),\n","        Bidirectional(LSTM(100)),\n","        Dropout(0.2),\n","        # Final classification layers\n","        Dense(100, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=100, batch_size=64,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nCNN-LSTM Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall CNN-LSTM Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall CNN-LSTM Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final CNN-LSTM Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final CNN-LSTM Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[-1])),\n","    Conv1D(filters=64, kernel_size=3, activation='relu'),\n","    Conv1D(filters=64, kernel_size=3, activation='relu'),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(100, return_sequences=True)),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(100)),\n","    Dropout(0.2),\n","    Dense(100, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_model.fit(X_seq, y_seq, epochs=30, batch_size=64, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      CNN-LSTM PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"]},{"cell_type":"code","execution_count":null,"id":"GOGVf7fVAdRk","metadata":{"id":"GOGVf7fVAdRk"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train Optimized Bidirectional LSTM with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Optimized Bi-LSTM Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Optimized Bidirectional LSTM Model ---\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[-1])),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(64)),\n","        Dropout(0.25),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    # Using a fine-tuned learning rate\n","    optimizer = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=150, # Increased epochs to allow for convergence with smaller batch size\n","        batch_size=32, # Using a smaller batch size\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nOptimized Bi-LSTM Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Optimized Bi-LSTM Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Optimized Bi-LSTM Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Optimized Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[-1])),\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(LSTM(64)),\n","    Dropout(0.25),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_optimizer = Adam(learning_rate=0.0005)\n","final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_model.fit(X_seq, y_seq, epochs=40, batch_size=32, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      OPTIMIZED PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Layer, Bidirectional, LSTM\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- Transformer Model Components ---\n","\n","class TransformerBlock(Layer):\n","    \"\"\"\n","    A custom Transformer block layer.\n","    This includes multi-head self-attention, feed-forward network, and layer normalization.\n","    \"\"\"\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n","        super(TransformerBlock, self).__init__(**kwargs)\n","        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = Sequential([\n","            Dense(ff_dim, activation=\"relu\"),\n","            Dense(embed_dim)\n","        ])\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training=False):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.att.key_dim,\n","            \"num_heads\": self.att.num_heads,\n","            \"ff_dim\": self.ffn.layers[0].units,\n","            \"rate\": self.dropout1.rate,\n","        })\n","        return config\n","\n","class TokenAndPositionEmbedding(Layer):\n","    \"\"\"\n","    A custom layer to add positional information to the input sequences.\n","    \"\"\"\n","    def __init__(self, maxlen, embed_dim, **kwargs):\n","        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n","        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.maxlen = maxlen\n","        self.embed_dim = embed_dim\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-2]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"maxlen\": self.maxlen,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Preprocess the Data ---\n","\n","# Separate features (X) and target (y)\n","X_df = df.drop('Crisis_Target_V2', axis=1)\n","y_df = df['Crisis_Target_V2']\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 3. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 4. Build and Train Transformer-LSTM Hybrid Model with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Transformer-LSTM Hybrid Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Transformer-LSTM Hybrid Model ---\n","    embed_dim = X_train.shape[-1]\n","    num_heads = 4\n","    ff_dim = 128\n","\n","    inputs = Input(shape=(time_step, embed_dim))\n","    embedding_layer = TokenAndPositionEmbedding(time_step, embed_dim)\n","    x = embedding_layer(inputs)\n","    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","    x = transformer_block(x)\n","    # Feed the output of the Transformer into a Bidirectional LSTM\n","    x = Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25))(x)\n","    x = Dense(64, activation=\"relu\")(x)\n","    x = Dropout(0.25)(x)\n","    outputs = Dense(num_classes, activation=\"softmax\")(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    optimizer = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=150,\n","        batch_size=32,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nTransformer-LSTM Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 5. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Transformer-LSTM Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Transformer-LSTM Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 6. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Transformer-LSTM Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","embed_dim = X_seq.shape[-1]\n","num_heads = 4\n","ff_dim = 128\n","\n","final_inputs = Input(shape=(time_step, embed_dim))\n","final_embedding_layer = TokenAndPositionEmbedding(time_step, embed_dim)\n","fx = final_embedding_layer(final_inputs)\n","final_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","fx = final_transformer_block(fx)\n","fx = Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25))(fx)\n","fx = Dense(64, activation=\"relu\")(fx)\n","fx = Dropout(0.25)(fx)\n","final_outputs = Dense(num_classes, activation=\"softmax\")(fx)\n","\n","final_model = Model(inputs=final_inputs, outputs=final_outputs)\n","final_optimizer = Adam(learning_rate=0.0005)\n","final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_model.fit(X_seq, y_seq, epochs=40, batch_size=32, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      TRANSFORMER-LSTM PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"],"metadata":{"id":"LJsrdHvVFWoI"},"id":"LJsrdHvVFWoI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_features_for_lstm.csv')\n","    target_df = pd.read_csv('/content/drive/Othercomputers/My Laptop (1)/E:/projects/water_crises_management/chennaiWaterCrisis/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Advanced Feature Engineering ---\n","print(\"\\n--- Performing Advanced Feature Engineering ---\")\n","\n","# Make a copy to avoid SettingWithCopyWarning\n","X_df = df.drop('Crisis_Target_V2', axis=1).copy()\n","y_df = df['Crisis_Target_V2']\n","\n","# a. Reservoir Capacity Percentage\n","reservoirs = ['CHEMBARAMBAKKAM', 'CHOLAVARAM', 'POONDI', 'PUZHAL', 'VEERANAM', 'KANNANKOTTAI_THERVOY_KANDIGAI']\n","for res in reservoirs:\n","    storage_col = f'Storage_mcft_{res}'\n","    capacity_col = f'Full_Capacity_mcft_{res}'\n","    if storage_col in X_df.columns and capacity_col in X_df.columns:\n","        # Add a small epsilon to avoid division by zero\n","        X_df[f'Capacity_perc_{res}'] = (X_df[storage_col] / (X_df[capacity_col] + 1e-6)) * 100\n","\n","# b. Lag Features for all reservoirs and key weather metrics\n","lag_features = []\n","for res in reservoirs:\n","    lag_features.append(f'Rainfall_mm_{res}')\n","    lag_features.append(f'Inflow_cusecs_{res}')\n","\n","lag_features.extend([\n","    'temperature_mean_celsius',\n","    'relative_humidity_mean_percent'\n","])\n","\n","lags = [1, 3, 7]\n","\n","for feature in lag_features:\n","    if feature in X_df.columns:\n","        for lag in lags:\n","            X_df[f'{feature}_lag_{lag}'] = X_df[feature].shift(lag)\n","    else:\n","        # This will skip features that don't exist (e.g., inflow for some reservoirs)\n","        pass\n","\n","\n","# c. Rolling Averages & Sums for all major reservoirs\n","rolling_features = []\n","for res in reservoirs:\n","    rolling_features.append(f'Rainfall_mm_{res}')\n","    rolling_features.append(f'Inflow_cusecs_{res}')\n","\n","windows = [7, 14, 30]\n","\n","for feature in rolling_features:\n","    if feature in X_df.columns:\n","        for window in windows:\n","            X_df[f'{feature}_roll_mean_{window}'] = X_df[feature].rolling(window=window).mean()\n","            X_df[f'{feature}_roll_sum_{window}'] = X_df[feature].rolling(window=window).sum()\n","    else:\n","        # This will skip features that don't exist (e.g., inflow for some reservoirs)\n","        pass\n","\n","\n","# d. Time-Based Features\n","X_df['month'] = X_df.index.month\n","X_df['week_of_year'] = X_df.index.isocalendar().week.astype(int)\n","X_df['day_of_year'] = X_df.index.dayofyear\n","\n","# e. Interaction Features\n","if 'temperature_mean_celsius' in X_df.columns and 'relative_humidity_mean_percent' in X_df.columns:\n","    X_df['temp_humidity_interaction'] = X_df['temperature_mean_celsius'] / (X_df['relative_humidity_mean_percent'] + 1e-6)\n","\n","\n","# Drop rows with NaN values created by feature engineering\n","X_df.dropna(inplace=True)\n","y_df = y_df.loc[X_df.index]\n","\n","print(f\"Shape after feature engineering and dropping NaNs: {X_df.shape}\")\n","\n","\n","# --- 3. Preprocess the Data ---\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Validate that labels are in the correct range [0, num_classes-1]\n","if y_df.min() < 0:\n","    print(f\"Error: Negative label found ({y_df.min()}). Labels must be non-negative.\")\n","    exit()\n","\n","\n","# Scale the features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 4. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 5. Build and Train Optimized Bidirectional LSTM with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Optimized Bi-LSTM Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Optimized Bidirectional LSTM Model ---\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[-1])),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(64)),\n","        Dropout(0.25),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    # Using a fine-tuned learning rate\n","    optimizer = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=150, # Increased epochs to allow for convergence with smaller batch size\n","        batch_size=32, # Using a smaller batch size\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nOptimized Bi-LSTM Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 6. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Optimized Bi-LSTM Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Optimized Bi-LSTM Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 7. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Optimized Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[-1])),\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(LSTM(64)),\n","    Dropout(0.25),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_optimizer = Adam(learning_rate=0.0005)\n","final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_model.fit(X_seq, y_seq, epochs=40, batch_size=32, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      OPTIMIZED PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"],"metadata":{"id":"CupKmSBcNqmS"},"id":"CupKmSBcNqmS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import TimeSeriesSplit\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- 1. Load and Prepare Data ---\n","\n","try:\n","    # Load the features and target datasets\n","    features_df = pd.read_csv(\"C:/Users/NMAMIT/Desktop/nnm22ad039/Datasets/Datasets/chennai_data_final_ENHANCED.csv\")\n","    target_df = pd.read_csv('C:/Users/NMAMIT/Desktop/nnm22ad039/Datasets/Datasets/chennai_target_for_lstm.csv')\n","\n","    # Convert 'Date' column to datetime objects\n","    features_df['Date'] = pd.to_datetime(features_df['Date'])\n","    target_df['Date'] = pd.to_datetime(target_df['Date'])\n","\n","    # Merge the two dataframes on the 'Date' column\n","    df = pd.merge(features_df, target_df, on='Date')\n","\n","    # Set 'Date' as the index\n","    df.set_index('Date', inplace=True)\n","\n","    print(\"Data loaded and merged successfully.\")\n","    print(\"DataFrame shape:\", df.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Please make sure 'chennai_features_for_lstm.csv' and 'chennai_target_for_lstm.csv' are in the same directory as the script.\")\n","    exit()\n","\n","\n","# --- 2. Advanced Feature Engineering ---\n","print(\"\\n--- Performing Advanced Feature Engineering ---\")\n","\n","# Make a copy to avoid SettingWithCopyWarning\n","X_df_full = df.drop('Crisis_Target_V2', axis=1).copy()\n","y_df = df['Crisis_Target_V2']\n","\n","# a. Reservoir Capacity Percentage\n","reservoirs = ['CHEMBARAMBAKKAM', 'CHOLAVARAM', 'POONDI', 'PUZHAL', 'VEERANAM', 'KANNANKOTTAI_THERVOY_KANDIGAI']\n","for res in reservoirs:\n","    storage_col = f'Storage_mcft_{res}'\n","    capacity_col = f'Full_Capacity_mcft_{res}'\n","    if storage_col in X_df_full.columns and capacity_col in X_df_full.columns:\n","        X_df_full[f'Capacity_perc_{res}'] = (X_df_full[storage_col] / (X_df_full[capacity_col] + 1e-6)) * 100\n","\n","# b. Lag Features for all reservoirs and key weather metrics\n","lag_features = []\n","for res in reservoirs:\n","    lag_features.append(f'Rainfall_mm_{res}')\n","    lag_features.append(f'Inflow_cusecs_{res}')\n","lag_features.extend(['temperature_mean_celsius', 'relative_humidity_mean_percent'])\n","lags = [1, 3, 7]\n","for feature in lag_features:\n","    if feature in X_df_full.columns:\n","        for lag in lags:\n","            X_df_full[f'{feature}_lag_{lag}'] = X_df_full[feature].shift(lag)\n","\n","# c. Rolling Averages & Sums for all major reservoirs\n","rolling_features = []\n","for res in reservoirs:\n","    rolling_features.append(f'Rainfall_mm_{res}')\n","    rolling_features.append(f'Inflow_cusecs_{res}')\n","windows = [7, 14, 30]\n","for feature in rolling_features:\n","    if feature in X_df_full.columns:\n","        for window in windows:\n","            X_df_full[f'{feature}_roll_mean_{window}'] = X_df_full[feature].rolling(window=window).mean()\n","            X_df_full[f'{feature}_roll_sum_{window}'] = X_df_full[feature].rolling(window=window).sum()\n","\n","# d. Time-Based Features\n","X_df_full['month'] = X_df_full.index.month\n","X_df_full['week_of_year'] = X_df_full.index.isocalendar().week.astype(int)\n","X_df_full['day_of_year'] = X_df_full.index.dayofyear\n","\n","# e. Interaction Features\n","if 'temperature_mean_celsius' in X_df_full.columns and 'relative_humidity_mean_percent' in X_df_full.columns:\n","    X_df_full['temp_humidity_interaction'] = X_df_full['temperature_mean_celsius'] / (X_df_full['relative_humidity_mean_percent'] + 1e-6)\n","\n","# Drop rows with NaN values created by feature engineering\n","X_df_full.dropna(inplace=True)\n","y_df = y_df.loc[X_df_full.index]\n","\n","print(f\"Shape after feature engineering: {X_df_full.shape}\")\n","\n","\n","# --- 3. Intelligent Feature Selection using XGBoost ---\n","print(\"\\n--- Performing Feature Selection with XGBoost ---\")\n","\n","# Train a simple XGBoost model to get feature importances\n","xgb_selector = xgb.XGBClassifier(objective='multi:softmax', use_label_encoder=False, eval_metric='mlogloss')\n","xgb_selector.fit(X_df_full, y_df)\n","\n","# Get feature importances\n","importances = xgb_selector.feature_importances_\n","feature_names = X_df_full.columns\n","\n","# Create a DataFrame for visualization\n","feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n","feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n","\n","# Select the top N features\n","N_FEATURES = 75\n","top_features = feature_importance_df.head(N_FEATURES)['feature'].tolist()\n","print(f\"Selected Top {N_FEATURES} features.\")\n","\n","# Create the final DataFrame with only the selected features\n","X_df = X_df_full[top_features]\n","\n","\n","# --- 4. Preprocess the Data ---\n","\n","# --- Diagnostic: Check Class Distribution ---\n","print(\"\\n--- Class Distribution Analysis ---\")\n","num_classes = len(y_df.unique())\n","print(f\"Number of unique crisis levels (classes): {num_classes}\")\n","print(f\"Unique target values: {np.sort(y_df.unique())}\")\n","print(\"\\nOverall class distribution in the entire dataset:\")\n","print(y_df.value_counts().sort_index())\n","\n","# Scale the selected features to be between 0 and 1.\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_scaled = scaler.fit_transform(X_df)\n","\n","# --- 5. Create Time-Series Sequences ---\n","def create_dataset(X, y, time_step=1):\n","    dataX, dataY = [], []\n","    for i in range(len(X) - time_step):\n","        a = X[i:(i + time_step), :]\n","        dataX.append(a)\n","        dataY.append(y[i + time_step])\n","    return np.array(dataX), np.array(dataY)\n","\n","time_step = 90\n","X_seq, y_seq = create_dataset(X_scaled, y_df.values, time_step)\n","\n","\n","# --- 6. Build and Train Optimized Bidirectional LSTM with Cross-Validation ---\n","\n","n_splits = 5\n","tscv = TimeSeriesSplit(n_splits=n_splits)\n","\n","fold_accuracies = []\n","all_y_test = []\n","all_y_pred = []\n","\n","print(f\"\\n--- Starting {n_splits}-Fold Time-Series Cross-Validation with Optimized Bi-LSTM Model ---\")\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X_seq)):\n","    print(f\"\\n===== FOLD {fold + 1}/{n_splits} =====\")\n","    X_train, X_test = X_seq[train_index], X_seq[test_index]\n","    y_train, y_test = y_seq[train_index], y_seq[test_index]\n","\n","    # --- Handle Class Imbalance with Class Weights ---\n","    class_weights = compute_class_weight(\n","        class_weight='balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_dict = dict(enumerate(class_weights))\n","    print(\"Class Weights for this fold:\", class_weights_dict)\n","\n","    # --- Build the Optimized Bidirectional LSTM Model ---\n","    model = Sequential([\n","        Input(shape=(time_step, X_train.shape[-1])),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(64)),\n","        Dropout(0.25),\n","        Dense(64, activation='relu'),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    optimizer = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n","\n","    model.fit(\n","        X_train, y_train,\n","        validation_data=(X_test, y_test),\n","        epochs=150,\n","        batch_size=32,\n","        class_weight=class_weights_dict,\n","        callbacks=[early_stopping],\n","        verbose=1\n","    )\n","    pred_probs = model.predict(X_test)\n","    y_pred = np.argmax(pred_probs, axis=1)\n","\n","    fold_accuracy = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(fold_accuracy)\n","    print(f\"\\nOptimized Bi-LSTM Accuracy for Fold {fold + 1}: {fold_accuracy:.4f}\")\n","\n","    all_y_test.extend(y_test)\n","    all_y_pred.extend(y_pred)\n","\n","\n","# --- 7. Final Model Evaluation ---\n","print(\"\\n\\n--- Overall Optimized Bi-LSTM Model Performance (Across All Folds) ---\")\n","print(f\"Average Cross-Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n","\n","# --- Overall Classification Report ---\n","print(\"\\nOverall Classification Report:\")\n","report = classification_report(all_y_test, all_y_pred, labels=np.sort(y_df.unique()), zero_division=0)\n","print(report)\n","\n","# --- Overall Confusion Matrix ---\n","print(\"\\n--- Plotting Overall Confusion Matrix ---\")\n","cm = confusion_matrix(all_y_test, all_y_pred, labels=np.sort(y_df.unique()))\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=np.sort(y_df.unique()),\n","            yticklabels=np.sort(y_df.unique()))\n","plt.title('Overall Optimized Bi-LSTM Confusion Matrix (All Folds)', fontsize=16, fontweight='bold')\n","plt.ylabel('Actual Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.show()\n","print(\"Confusion Matrix plot displayed.\")\n","\n","\n","# --- 8. Train Final Model and Make Prediction for Tomorrow ---\n","\n","print(\"\\n\\n--- Training Final Optimized Model on All Data for Prediction ---\")\n","\n","# --- Final Class Weights ---\n","final_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_seq), y=y_seq)\n","final_class_weights_dict = dict(enumerate(final_class_weights))\n","\n","# --- Build the final model ---\n","final_model = Sequential([\n","    Input(shape=(time_step, X_seq.shape[-1])),\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(LSTM(128, return_sequences=True)),\n","    Dropout(0.25),\n","    Bidirectional(LSTM(64)),\n","    Dropout(0.25),\n","    Dense(64, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","final_optimizer = Adam(learning_rate=0.0005)\n","final_model.compile(optimizer=final_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# --- Train the final model ---\n","final_model.fit(X_seq, y_seq, epochs=40, batch_size=32, class_weight=final_class_weights_dict, verbose=1)\n","\n","# --- Prepare input for prediction ---\n","last_days_scaled = X_scaled[-time_step:]\n","prediction_input = last_days_scaled.reshape(1, time_step, X_df.shape[1])\n","\n","# --- Make final prediction ---\n","final_pred_probs = final_model.predict(prediction_input)\n","predicted_class = np.argmax(final_pred_probs, axis=1)[0]\n","\n","# --- Display the prediction ---\n","last_date_in_data = X_df.index[-1]\n","prediction_date = last_date_in_data + pd.Timedelta(days=1)\n","\n","print(\"\\n\\n=====================================================\")\n","print(f\"      OPTIMIZED PREDICTION FOR CHENNAI WATER CRISIS\")\n","print(\"=====================================================\")\n","print(f\"Based on data up to: {last_date_in_data.strftime('%Y-%m-%d')}\")\n","print(f\"Predicted Crisis Level for {prediction_date.strftime('%Y-%m-%d')}: {predicted_class}\")\n","print(\"=====================================================\")\n","print(\"\\nCrisis Levels:\")\n","print(\"0: No Crisis\")\n","print(\"1: Moderate Crisis\")\n","print(\"2: Severe Crisis\")\n","print(\"3: Extreme Crisis\")\n"],"metadata":{"id":"HWP-4iIIzscV"},"id":"HWP-4iIIzscV","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"V28","private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}